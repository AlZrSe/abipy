{
 "metadata": {
  "name": "",
  "signature": "sha256:2b643d3aac3722335d068041e8fad914eddca47a1e1ded9b578915c4ec971a34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Back to the main [Index](../index.ipynb)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tasks, Workflows and Flow"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before starting it's worth clarifying some of the basic concepts used in `AbiPy` to automate ab-initio calculations. \n",
      "In particular, in this paragraph we will focus on the following three objects: \n",
      "\n",
      "   * `Task`\n",
      "   * `Work`\n",
      "   * `Flow`\n",
      "   \n",
      "The `Task` represent the most elementary step of the calculation. \n",
      "Roughly speaking, it corresponds to the execution of an ABINIT run.\n",
      "From the point of view of `Abipy`, a calculation consists of a set of `Tasks` that are connected together by some sort of dependency. \n",
      "Each task has a list of files that are needed to start the calculation, and a list of files that are produced at the end of the run.\n",
      "Some of the input files needed by a `Task` must be provided by the user in the form of ABINIT input variables \n",
      "(e.g. the crystalline structure), other inputs (files) may be produced by other tasks.\n",
      "When a `Task` B requires the output file `DEN` of another calculation A, we say that B depends on A through F and we express this dependency using the python dictionary  \n",
      "\n",
      "```python\n",
      "B_deps = {A: \"DEN\"}\n",
      "```\n",
      "\n",
      "The `Work` can be seen as a (dynamic) list of `Tasks` and `AbiPy` provides several pre-built \n",
      "factory functions for typical first-principles calculations.\n",
      "Finally, the `Flow` is a list of `Work` objects whose main goal is providing an easy-to-use\n",
      "interface for performing common operations. \n",
      "\n",
      "To clarify this point, let's take a standard $KS$ band structure calculation as example.\n",
      "In this case, we have an initial `ScfTask` that solves the KS equations self-consistently to produce a density file. \n",
      "The density is then used by a second `NscfTask` to produce a band structure on an arbitrary \n",
      "set of $k$-points.\n",
      "The `NscfTask` has thus a dependency on the first `ScfTask` in the sense that it cannot be executed until \n",
      "the `ScfTask` is completed, and the `DEN` file produced by the `ScfTask` is needed as input.\n",
      "\n",
      "All the `Works` and the `Tasks` of a flow are created and executed inside the `Flow` working directory (`workdir`) \n",
      "that is usually specified by the user during the creation of the `Flow`.\n",
      "`Abipy` will then create the workdir of the different Works/Tasks when the `Flow` is executed\n",
      "for the first time.\n",
      "\n",
      "Each `Task` has an associated set of input variables that will be used to generate the ABINIT input file and run the calculation. \n",
      "This input **must** be provided by the user during the creation of the `Task`.\n",
      "Fortunately `AbiPy` provides an object named `AbiInput` that facilitates the creation of such input. \n",
      "Once you have an `AbiInput` object, you can create the corresponding `Task`.\n",
      "In pseudo code:\n",
      "\n",
      "```python\n",
      "new_task = Task(abi_input)\n",
      "```\n",
      "\n",
      "The `Task` provides several method for monitoring the status of the ABINIT run and perform some \n",
      "analysis or post-processing of the results.\n",
      "\n",
      "Fortunately you do not need to understand all the technical details of the python implementation.\n",
      "In many cases, indeed, we already provide some kind of `Work` or `Flow` that automates \n",
      "your calculation and you only have to provide the correct list of input files \n",
      "that obviously must be consistent with the Flow/Work that is being created.\n",
      "(you shall not pass a list of inputs for performing a band structure calculation to a flow \n",
      "that is expected to compute phonons with DFPT!\n",
      "\n",
      "In the next paragrap, we discuss how to construct a `Flow` for band-structure calculations\n",
      "with a high-level interface that only requires the specifications on the input files.\n",
      "This example allows us to discuss the most important methods of the `Flow`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a `Flow` for the band structure of Silicon."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by creating a functions that will produce two input files. \n",
      "The first input is a standard self-consistent ground-state calculation. \n",
      "In the example, we use `ndtset=2` simply because this trick allows us to reduce the number of calls we have to perform to initialize the variables of the calculations. For example we set the structure only once by calling `inp.set_structure_from_file` so that both dataset1 and dataset2 will use the same crystalline parameters.\n",
      "The function `split_datasets` returns **two inputs files** that can be run separately. \n",
      "The second dataset uses the density produced in the first run to perform a non self-consistent band structure calculation.\n",
      "**Note** that we don't have to use `getden2 = -1` in the second dataset since `Abipy` knows how to connect the two steps."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we have all the configuration files needed for running our calculations and we can start to write the python code that will create our calculation. Let's start by defining a function that returns two Abinit input files: one for the ground-state calculation and the other one for the NSCF run for the band structure calculation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function, unicode_literals\n",
      "\n",
      "from abipy import abilab\n",
      "import abipy.data as abidata\n",
      "\n",
      "def make_scf_nscf_inputs():\n",
      "    \"\"\"Build ands return the input files for the GS-SCF and the GS-NSCF tasks.\"\"\"\n",
      "    inp = abilab.AbiInput(pseudos=abidata.pseudos(\"14si.pspnc\"), ndtset=2)\n",
      "    inp.set_structure(abidata.cif_file(\"si.cif\"))\n",
      "\n",
      "    # Set global variables (dataset1 and dataset2)\n",
      "    inp.set_vars(ecut=6, nband=8)\n",
      "\n",
      "    # Dataset 1 (GS-SCF run)\n",
      "    inp[1].set_kmesh(ngkpt=[8,8,8], shiftk=[0,0,0])\n",
      "    inp[1].set_vars(tolvrs=1e-6)\n",
      "\n",
      "    # Dataset 2 (GS-NSCF run on a k-path)\n",
      "    kptbounds = [\n",
      "        [0.5, 0.0, 0.0], # L point\n",
      "        [0.0, 0.0, 0.0], # Gamma point\n",
      "        [0.0, 0.5, 0.5], # X point\n",
      "    ]\n",
      "\n",
      "    inp[2].set_kpath(ndivsm=6, kptbounds=kptbounds)\n",
      "    inp[2].set_vars(tolwfr=1e-12)\n",
      "    \n",
      "    # Return two input files for the GS and the NSCF run\n",
      "    scf_input, nscf_input = inp.split_datasets()\n",
      "    return scf_input, nscf_input"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have our two input files, we can call our function to get to the input variables and pass the two input files to the factory function `bandstructure_flow` that will return our `Flow`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scf_input, nscf_input = make_scf_nscf_inputs()\n",
      "\n",
      "workdir = \"/tmp/hello_bands\"\n",
      "flow = abilab.bandstructure_flow(workdir, scf_input, nscf_input)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`bandstructure_flow` took care of creating the correct dependency between the two tasks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.show_dependencies()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<ScfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t0>\u001b[0m\n",
        "\n",
        "<NscfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t1>\u001b[0m\n",
        "  +--<ScfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t0>\u001b[0m\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `NscfTask`, indeed,  depends on the `ScfTask` in w0/t0, whereas the `ScfTask` has no dependency."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To have useful info on status of the flow, use:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.show_status()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=\u001b[32mTrue\u001b[0m\n",
        "\n",
        "+------+-----------+-----------+---------------------+---------------+----------+---------+---------+\n",
        "| Task |   Status  |   Queue   | MPI|OMP|Memproc[Gb] | Err|Warn|Comm |  Class   | Restart | Node_ID |\n",
        "+------+-----------+-----------+---------------------+---------------+----------+---------+---------+\n",
        "|  t0  | \u001b[32mCompleted\u001b[0m | 1215@gmac |       2|1|2.0       |    NA|NA|NA   | ScfTask  |    0    |  58661  |\n",
        "|  t1  | \u001b[32mCompleted\u001b[0m | 1224@gmac |       2|1|2.0       |    NA|NA|NA   | NscfTask |    0    |  58662  |\n",
        "+------+-----------+-----------+---------------------+---------------+----------+---------+---------+\n",
        "\n",
        "all_ok reached\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Meaning of the different columns:\n",
      "    \n",
      "   * Task: short name of the task (usually t[index_in_work]\n",
      "   * Status: Status of the task\n",
      "   * Queue: Job identifier returned by the resource manager when the task is submitted\n",
      "   * (MPI| OMP |Memproc): Number of MPI procs, OMP threads, and memory per MPI proc\n",
      "   * Err|Warn|Comm: Number of Error/Warning/Comment messages found in the ABINIT log\n",
      "   * Class: The class of the `Task`\n",
      "   * Restart: Number of restart performed\n",
      "   * Node_ID : identifier of the task, used to select tasks or works in python code or `abirun.py`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to build and run the `Flow`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The flow is still in memory and no file has been produced. In order to build the workflow, we use "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "retcode = flow.build_and_pickle_dump()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function creates the directories of the `Flow`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!tree /tmp/hello_bands"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/tmp/hello_bands\r\n",
        "\u251c\u2500\u2500 __AbinitFlow__.pickle\r\n",
        "\u251c\u2500\u2500 indata\r\n",
        "\u251c\u2500\u2500 outdata\r\n",
        "\u251c\u2500\u2500 tmpdata\r\n",
        "\u2514\u2500\u2500 w0\r\n",
        "    \u251c\u2500\u2500 indata\r\n",
        "    \u251c\u2500\u2500 outdata\r\n",
        "    \u251c\u2500\u2500 t0\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u251c\u2500\u2500 t1\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u2514\u2500\u2500 tmpdata\r\n",
        "\r\n",
        "15 directories, 7 files\r\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`w0` is the directory containing the input files of the first workflow (well, we have only one workflow in our example).\n",
      "`t0` and `t1` contain the input files need to run the SCF and the NSC run, respectively.\n",
      "You might have noticed that each `Task` directory present the same structure:\n",
      "    \n",
      "   * run.abi: ABINIT input file\n",
      "   * run.files: ABINIT files file\n",
      "   * job.sh: Submission script\n",
      "   * outdata: Directory containing output data files\n",
      "   * indata: Directory containing input data files \n",
      "   * tmpdata: Directory with temporary files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-error\">\n",
      " `__AbinitFlow__.pickle` is the pickle database used to store the `Flow` we have just created. **Don't touch it!** \n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Executing a `Flow`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we only need to run our calculations with the command:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.make_scheduler().start()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=\u001b[32mTrue\u001b[0m\n",
        "\n",
        "all_ok reached\n",
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=\u001b[32mTrue\u001b[0m\n",
        "\n",
        "all_ok reached\n",
        "Submitted on Wed Feb 11 00:41:23 2015\n",
        "Completed on Wed Feb 11 00:41:28 2015\n",
        "Elapsed time 0:00:05.070994\n",
        "Flow completed successfully\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the first iteration, only the `ScfTask` is executed because the second task depends on it. \n",
      "At this point the scheduler will regularly wakes up, inspect the Tasks in the flow and \n",
      "submit those whose dependency is fulfilled."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "More on `Works` and `Tasks` "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both `Flow` and `Work` are iterable. \n",
      "Iterating on a `Flow` gives `Work` objects, whereas\n",
      "iterating over a `Work` will give the `Tasks` inside that particular `Work`.\n",
      "\n",
      "```python\n",
      "for work in flow:\n",
      "    for task in work:\n",
      "        print(task)\n",
      "```\n",
      "\n",
      "One can also \"slice\" `Flows` and `Works` i.e. one can select a slice or works or tasks\n",
      "with the syntax: flow[start:stop] or work[start:stop]\n",
      "This means that the previous loop is equivalent to the much more verbose version: \n",
      "\n",
      "```python\n",
      "for i in range(len(flow)):\n",
      "    work = flow[i]\n",
      "    for t in range(len(work):\n",
      "        print(work[t])\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TaskManager"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous sections, we have discussed how to define, build and run a `Flow`, but there is a very important point that we haven't discussed yet.\n",
      "It should be stressed, indeed, that `AbiPy` is only driving and monitoring the `Flow` while the actual calculation is delegated to ABINIT (a Fortran program that is usually executed in parallel on multiple CPUs that communicate via the network by means of the MPI protocol).\n",
      "Besides CPUs and memory must be reserved in advance by sending a request to the resource manager installed on the clusters (SLURM, PBS, etc)\n",
      "\n",
      "`AbiPy` gets all the information needed to submit the different `Tasks` from a configuration file, `manager.yml`, that is usually located in the directory `~/.abinit/abipy/`. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "For a brief introduction to the YAML format, please consult http://en.wikipedia.org/wiki/YAML\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`manager.yml` contains a list of `QueueAdapters` objects. \n",
      "Each `QueueAdapter` is responsible for all interactions with a specific queue management system (bash, slurm, PBS etc).\n",
      "This includes handling all details of queue script format as well as queue submission and management.\n",
      "The configuration file I use on my laptop is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qadapters:\n",
      "    # List of qadapters objects \n",
      "    - priority: 1\n",
      "      queue:\n",
      "        qtype: shell\n",
      "        qname: gmac\n",
      "      job:\n",
      "        mpi_runner: mpirun\n",
      "        pre_run: \"source ~/env.sh\"\n",
      "      limits:\n",
      "        timelimit: 1:00:00\n",
      "        max_cores: 2\n",
      "      hardware:\n",
      "         num_nodes: 1\n",
      "         sockets_per_node: 1\n",
      "         cores_per_socket: 2\n",
      "         mem_per_node: 4 Gb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the sake of brevity, we just try to give you a general overview of the meaning \n",
      "of the different sections without entering into detail.\n",
      " \n",
      "* queue: dictionary with the name of the queue and optional parameters \n",
      "     used to build/customize the header of the submission script.\n",
      "* job: dictionary with the options used to prepare the enviroment before submitting the job\n",
      "* limits: dictionary with the constraints that must be fulfilled in order to run with this queueadapter.\n",
      "* hardware: dictionary with information on the hardware available on this particular queue.\n",
      "\n",
      "In this (simple) case, we have one `QueueAdapter` named `gmac` that will submit `Tasks`\n",
      "in a shell subprocess (`qtype: shell`) via `mpirun`. \n",
      "`env.sh` is the bash script I use to set the value of the environment variables (e.g. `PATH` and `LD_LIBRARY_PATH`) before running ABINIT.\n",
      "\n",
      "Note that my laptopt has 1 socket with 2 CPUs and 4 Gb of memory in total, hence I don't want\n",
      "to run an ABINIT tasks with more than 2 CPUs. This is the reason why `max_cores` is set to 2.\n",
      "Timelimit is not used when you are using `qname=shell` but it is very important when you are submitting jobs on a cluster because this value is used to generate the submission script.\n",
      "\n",
      "At this point, you may wonder why we need to specify all these parameters in the configuration file.\n",
      "The reason is that, before submitting a job to a resource manager, `AbiPy` will use the autoparal feature of ABINIT to get all the possible parallel configurations with ncpus <= max_cores. \n",
      "On the basis of these results, `AbiPy` selects the \"optimal\" one and changes the ABINIT input file and the submission script sccordingly \n",
      "(this is very useful feature, especially for calculations done with `paral_kgb=1` that require the specification of `npkpt`, `npfft`, `npband`, etc.).\n",
      "If more than one `QueueAdapter` is specified, `AbiPy` will first compute all the possible configuration and then select the \"optimal\" `QueueAdapter` according to some kind of policy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The complete list of options supported by the `TaskManager` can be retrieved with the command:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!abirun.py docmanager"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "# TaskManager configuration file (YAML Format)\r\n",
        "# Main options:\r\n",
        "\r\n",
        "policy: \r\n",
        "\r\n",
        "qadapters:  \r\n",
        "    # List of qadapters objects (mandatory)\r\n",
        "    -  # qadapter_1\r\n",
        "    -  # qadapter_2\r\n",
        "\r\n",
        "db_connector: # Connection to MongoDB database (optional)\r\n",
        "\r\n",
        "##########################################\r\n",
        "# Individual entries are documented below:\r\n",
        "##########################################\r\n",
        "\r\n",
        "qadapter: \r\n",
        "# dictionary with infor on the hardware available on this particular queue.\r\n",
        "hardware:  \r\n",
        "    num_nodes:        # Number of nodes available on this queue. Mandatory\r\n",
        "    sockets_per_node: # Self-explanatory. Mandatory.\r\n",
        "    cores_per_socket: # Self-explanatory. Mandatory.\r\n",
        "\r\n",
        "# dictionary with the options used to prepare the enviroment before submitting the job\r\n",
        "job:\r\n",
        "    setup:       # List of commands (str) executed before running (default empty)\r\n",
        "    omp_env:     # Dictionary with OpenMP env variables (default empty i.e. no OpenMP)\r\n",
        "    modules:     # List of modules to be imported (default empty)\r\n",
        "    shell_env:   # Dictionary with shell env variables.\r\n",
        "    mpi_runner:  # MPI runner i.e. mpirun, mpiexec, Default is None i.e. no mpirunner\r\n",
        "    pre_run:     # List of commands executed before the run (default: empty)\r\n",
        "    post_run:    # List of commands executed after the run (default: empty)\r\n",
        "\r\n",
        "# dictionary with the name of the queue and optional parameters \r\n",
        "# used to build/customize the header of the submission script.\r\n",
        "queue:\r\n",
        "    qname:   # Name of the queue (mandatory)\r\n",
        "    qparams: # Dictionary with values used to generate the header of the job script\r\n",
        "             # See pymatgen.io.abinitio.qadapters.py for the list of supported values.\r\n",
        "\r\n",
        "# dictionary with the constraints that must be fulfilled in order to run on this queue.\r\n",
        "limits:\r\n",
        "    min_cores:         # Minimum number of cores (default 1)\r\n",
        "    max_cores:         # Maximum number of cores (mandatory)\r\n",
        "    min_mem_per_proc:  # Minimum memory per MPI process in megabytes, units can be specified e.g. 1.4 Gb\r\n",
        "                       # (default hardware.mem_per_core)\r\n",
        "    max_mem_per_proc:  # Maximum memory per MPI process in megabytes, units can be specified e.g. `1.4Gb`\r\n",
        "                       # (default hardware.mem_per_node)\r\n",
        "    condition:         # MongoDB-like condition (default empty, i.e. not used)\r\n",
        "\r\n",
        "db_connector: \r\n",
        "     enabled:     # yes or no (default yes)\r\n",
        "     database:    # Name of the mongodb database (default abinit)\r\n",
        "     collection:  # Name of the collection (default test)\r\n",
        "     host:        # host address e.g. 0.0.0.0 (default None)\r\n",
        "     port:        # port e.g. 8080 (default None)\r\n",
        "     user:        # user name (default None)\r\n",
        "     password:    # password for authentication (default None)\r\n",
        "     \r\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "`Abipy` will read the `manager.yml` file automatically when a new `Flow` is created. \n",
      "By default, python looks for a `manager.yml` file in the current working directory and \n",
      "then in `~/.abinit/abipy`. \n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scheduler"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other configuration file is named `scheduler.yml` and defines the parameters \n",
      "for the scheduler that will run/submit our jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ~/.abinit/abipy/scheduler.yml"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# number of weeks to wait.\r\n",
        "#weeks:\r\n",
        "# number of days to wait.\r\n",
        "#days:\r\n",
        "# number of hours to wait.\r\n",
        "#hours:\r\n",
        "# number of minutes to wait.\r\n",
        "#minutes:\r\n",
        "# number of seconds to wait.\r\n",
        "seconds: 5\r\n",
        "max_nlaunches: 2\r\n",
        "# when to first execute the job and start the counter (default is after the given interval).\r\n",
        "#start_date:\r\n",
        "# Send mail to the specified address (accepts string or list of strings).\r\n",
        "#mailto: nobody@nowhere.com \r\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "Also in this case, `Abipy` will read the `scheduler.yml` file automatically when `make_scheduler` \n",
      "is called without arguments. \n",
      "A file `manager.yml` located in the current working directory takes precedence over `~/.abinit/abipy/manager.yml`. \n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-error\">\n",
      "Remember to set the time interval of the scheduler to a reasonable value.\n",
      "A small value leads to an increase of the submission rate but it also increases the CPU load \n",
      "and the pressure on the hardware and on the resource manager.\n",
      "A too large time interval can have a detrimental effect on the throughput, especially \n",
      "if you are submitting many small jobs.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Abirun.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Executing `flow.make_scheduler().start()` inside an ipython notebook is handy if you are dealing with small calculations that require few seconds or minutes. \n",
      "This approach, however, is unpractical when you have large flows or big calculations requiring hours or days, even on massively parallel machines.\n",
      "In this case, indeed, you would like to run the scheduler in a separate process in the backgroud so that the scheduler is not killed when you close the ipython notebook server.\n",
      "\n",
      "To start the scheduler in a separate process, use the `abirun.py` script.\n",
      "The syntax is \n",
      "\n",
      "```bash\n",
      "abirun.py flowdir command\n",
      "```\n",
      "\n",
      "where `flowdir` is the directory containing the `Flow` (the directory with the pickle file) and `command` select the operation to be performed.\n",
      "\n",
      "Typical examples:\n",
      "\n",
      "```bash\n",
      "abirun.py /tmp/hello_bands status\n",
      "```\n",
      "checks the status of the `Flow` and print the results to screen.\n",
      "\n",
      "```bash\n",
      "nohup abirun.py /tmp/hello_bands scheduler > sched.log &\n",
      "```\n",
      "\n",
      "Starts the scheduler in the background redirecting the standard output to file `sched.log`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"alert alert-success\">\n",
      "`nohup` is a standard Unix tool. The command make the scheduler immune \n",
      "to hangups so that you can close the shell session without killing the scheduler.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`--help` gives the complete list of commands and options available: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!abirun.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "usage: abirun.py [-h] [-v] [--remove-lock REMOVE_LOCK] [--no-colors NO_COLORS] [--loglevel LOGLEVEL]\r\n",
        "                 [path]\r\n",
        "                 {single,rapid,scheduler,status,cancel,restart,reset,move,open,ncopen,gui,new_manager,tail,qstat,deps,robot,plot,inspect,inputs,analyze,docmanager,notebook,ipython}\r\n",
        "                 ...\r\n",
        "\r\n",
        "positional arguments:\r\n",
        "  path                  File or directory containing the ABINIT flow If not given, the first flow in the current workdir\r\n",
        "                        is selected\r\n",
        "\r\n",
        "optional arguments:\r\n",
        "  -h, --help            show this help message and exit\r\n",
        "  -v, --verbose         verbose, can be supplied multiple times to increase verbosity\r\n",
        "  --remove-lock REMOVE_LOCK\r\n",
        "                        Remove the lock file of the pickle file storing the flow.\r\n",
        "  --no-colors NO_COLORS\r\n",
        "                        Disable ASCII colors\r\n",
        "  --loglevel LOGLEVEL   set the loglevel. Possible values: CRITICAL, ERROR (default), WARNING, INFO, DEBUG\r\n",
        "\r\n",
        "subcommands:\r\n",
        "  Valid subcommands\r\n",
        "\r\n",
        "  {single,rapid,scheduler,status,cancel,restart,reset,move,open,ncopen,gui,new_manager,tail,qstat,deps,robot,plot,inspect,inputs,analyze,docmanager,notebook,ipython}\r\n",
        "                        sub-command help\r\n",
        "    single              Run single task.\r\n",
        "    rapid               Run all tasks in rapidfire mode\r\n",
        "    scheduler           Run all tasks with a Python scheduler.\r\n",
        "    status              Show task status.\r\n",
        "    cancel              Cancel the tasks in the queue.\r\n",
        "    restart             Restart the tasks of the flow that are not converged.\r\n",
        "    reset               Reset the tasks of the flow with the specified status.\r\n",
        "    move                Move the flow to a new directory and change the absolute paths\r\n",
        "    open                Open files in $EDITOR, type `abirun.py DIRPATH open --help` for help)\r\n",
        "    ncopen              Open netcdf files in ipython, type `abirun.py DIRPATH ncopen --help` for help)\r\n",
        "    gui                 Open GUI.\r\n",
        "    new_manager         Change the TaskManager.\r\n",
        "    tail                Use tail to follow the main output file of the flow.\r\n",
        "    qstat               Show additional info on the jobs in the queue.\r\n",
        "    deps                Show dependencies.\r\n",
        "    robot               Use a robot to analyze the results of multiple tasks (requires ipython)\r\n",
        "    plot                Plot data\r\n",
        "    inspect             Inspect the tasks\r\n",
        "    inputs              Show the input files of the tasks\r\n",
        "    analyze             Analyze the results produced by the flow (requires a flow with analyze method)\r\n",
        "    docmanager          Document the TaskManager options\r\n",
        "    notebook            Create and open an ipython notebook to interact with the flow.\r\n",
        "    ipython             Embed IPython. Useful for performing advanced operations or debugging purposes. THIS OPTION IF\r\n",
        "                        FOR EXPERT USERS!\r\n",
        "\r\n",
        "Usage example:\r\n",
        "\r\n",
        "    abirun.py [DIRPATH] single                   => Fetch the first available task and run it.\r\n",
        "    abirun.py [DIRPATH] rapid                    => Keep repeating, stop when no task can be executed\r",
        "\r\n",
        "                                                    due to inter-dependency.\r\n",
        "    abirun.py [DIRPATH] gui                      => Open the GUI \r\n",
        "    nohup abirun.py [DIRPATH] sheduler -s 30 &   => Use a scheduler to schedule task submission\r\n",
        "\r\n",
        "    If DIRPATH is not given, abirun.py automatically selects the database located within \r\n",
        "    the working directory. An Exception is raised if multiple databases are found.\r\n",
        "\r\n",
        "    Options for developers:\r\n",
        "        abirun.py prof ABIRUN_OPTIONS      to profile abirun.py\r\n",
        "        abirun.py tracemalloc ABIRUN_ARGS  to trace memory blocks allocated by Python\r\n"
       ]
      }
     ],
     "prompt_number": 38
    }
   ],
   "metadata": {}
  }
 ]
}