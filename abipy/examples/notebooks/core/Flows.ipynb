{
 "metadata": {
  "name": "",
  "signature": "sha256:c8822b42af58a4fa82f4c4d08071f26035aaa5c89a3ec5c278efbd7c9a5e3a50"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tasks, Workflows and Flow"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before starting it's worth clarifying some of the basic concepts used in `AbiPy` to automate ab-initio calculations. \n",
      "In particula, in this paragraph we will focus on the following objects: \n",
      "\n",
      "   * Tasks\n",
      "   * Works\n",
      "   * Flow\n",
      "   \n",
      "The `Task` represent the most elementary step of the calculation. \n",
      "Roughly speaking, it corresponds to the execution of an ABINIT run.\n",
      "From the point of view of `Abipy`, a calculation consists of a set of `Tasks` that are connected together by some sort of dependency. \n",
      "Each task has a (possibly empty) list of files that are needed to start the calculation, and a list of files that are produced at the end of the run.\n",
      "Some of the input files needed by a `Task` must be provided by the user in the form of ABINIT input variables (e.g. the crystalline structure), other inputs (files) may be produced by othe tasks.\n",
      "\n",
      "Each task is executed inside a unique working directory (`workdir`) whose structure is \n",
      "\n",
      "There are many possible Tasks available in `AbiPy`\n",
      "\n",
      "The `Work` can be seen as a (dynamic) list of Tasks and `AbiPy` provides several pre-built \n",
      "works for typical first-principles calculations (e.g. a bands-structure work contains a `ScfTask`\n",
      "followed by a `NscfTask`).\n",
      "\n",
      "Finally, the `Flow` is a list of `Work` objects whose main goal is providing an easy-to-use\n",
      "interface for performing common operations. \n",
      "End-users will mainly interact with the flow.\n",
      "\n",
      "\n",
      "To clarify this point, let's take a standard KS band structure calculation as example.\n",
      "In this case we have an initial `ScfTask` that solves the KS equations self-consistently to produce a density file. \n",
      "The density is then used by a second `NscfTask` to produce a band structure on an arbitrary \n",
      "set of $k$-points.\n",
      "The `NscfTask` has thus a dependency on the first `ScfTask`in the sense that it cannot be executed until the `ScfTask` is completed and the `DEN` file produced by the `ScfTask` is needed as input."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a `Flow` for the band structure of Silicon."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by creating a functions that will produce two input files. \n",
      "The first input is a standard self-consistent ground-state calculation. \n",
      "In the example, we use `ndtset=2` simply because this trick allows us to reduce the number of calls we have to perform to initialize the variables of the calculations. For example we set the structure only once by calling `inp.set_structure_from_file` so that both dataset1 and dataset2 will use the same crystalline parameters.\n",
      "The function `split_datasets` returns **two inputs files** that can be run separately. \n",
      "The second dataset uses the density produced in the first run to perform a non self-consistent band structure calculation.\n",
      "**Note** that we don't have to use `getden2 = -1` in the second dataset since `Abipy` knows how to connect the two steps."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Costructing the `Flow`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we have all the configuration files needed for running our calculations and we can start to write the python code that will create our calculation. Let's start by defining a function that returns two Abinit input files: one for the ground-state calculation and the other one for the NSCF run for the band structure calculation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function, unicode_literals\n",
      "\n",
      "import os\n",
      "from abipy import abilab\n",
      "import abipy.data as abidata\n",
      "\n",
      "def make_scf_nscf_inputs():\n",
      "    \"\"\"Build ands return the input files for the GS-SCF and the GS-NSCF tasks.\"\"\"\n",
      "    inp = abilab.AbiInput(pseudos=abidata.pseudos(\"14si.pspnc\"), ndtset=2)\n",
      "    inp.set_structure(abidata.cif_file(\"si.cif\"))\n",
      "\n",
      "    # Set global variables (dataset1 and dataset2)\n",
      "    inp.set_variables(ecut=6, nband=8)\n",
      "\n",
      "    # Dataset 1 (GS-SCF run)\n",
      "    inp[1].set_kmesh(ngkpt=[8,8,8], shiftk=[0,0,0])\n",
      "    inp[1].set_vars(tolvrs=1e-6)\n",
      "\n",
      "    # Dataset 2 (GS-NSCF run on a k-path)\n",
      "    kptbounds = [\n",
      "        [0.5, 0.0, 0.0], # L point\n",
      "        [0.0, 0.0, 0.0], # Gamma point\n",
      "        [0.0, 0.5, 0.5], # X point\n",
      "    ]\n",
      "\n",
      "    inp[2].set_kpath(ndivsm=6, kptbounds=kptbounds)\n",
      "    inp[2].set_vars(tolwfr=1e-12)\n",
      "    \n",
      "    # Generate two input files for the GS and the NSCF run\n",
      "    scf_input, nscf_input = inp.split_datasets()\n",
      "    return scf_input, nscf_input"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have our two input files we can call our function to get to the input variables and pass the two input files to the factory function `bandstructure_flow` that will construct \n",
      "an object describing a band structure calculation with Abinit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bands_flow(workdir):\n",
      "    # Call our function to get the two inputs.\n",
      "    scf_input, nscf_input = make_scf_nscf_inputs()\n",
      "    \n",
      "    # Read the TaskManager from the configuration file \"taskmanager.yml\"\n",
      "    manager = abilab.TaskManager.from_user_config()\n",
      "    \n",
      "    # Call the factory function for band structure flows provided by abilab\n",
      "    # A flow with one work made of a ScfTask + NscfTask.\n",
      "    flow = abilab.bandstructure_flow(workdir, scf_input, nscf_input)\n",
      "    return flow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we can construct our band structure flow by just passing to `bands_flow` the name of the directory where we want to produce the results: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow = bands_flow(workdir=\"/tmp/hello_bands\")\n",
      "flow.show_status()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=False\n",
        "\n",
        "+------+-------------+-------+---------------------+---------------+----------+---------+---------+\n",
        "| Task |    Status   | Queue | MPI|OMP|Memproc[Gb] | Err|Warn|Comm |  Class   | Restart | Node_ID |\n",
        "+------+-------------+-------+---------------------+---------------+----------+---------+---------+\n",
        "|  t0  | Initialized\u001b[0m |  None |       1|1|2.0       |    NA|NA|NA   | ScfTask  |    0    |  58602  |\n",
        "|  t1  | Initialized\u001b[0m |  None |       1|1|2.0       |    NA|NA|NA   | NscfTask |    0    |  58603  |\n",
        "+------+-------------+-------+---------------------+---------------+----------+---------+---------+\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.show_dependencies()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<ScfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t0>\u001b[0m\n",
        "\n",
        "<NscfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t1>\u001b[0m\n",
        "  +--<ScfTask, workdir=../../../../../../../../../tmp/hello_bands/w0/t0>\u001b[0m\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to build and run the `Flow`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The flow is still in memory and no file has been produced. In order to build the workflow, we have to use "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.build_and_pickle_dump()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!tree /tmp/hello_bands"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/tmp/hello_bands\r\n",
        "\u251c\u2500\u2500 __AbinitFlow__.pickle\r\n",
        "\u251c\u2500\u2500 indata\r\n",
        "\u251c\u2500\u2500 outdata\r\n",
        "\u251c\u2500\u2500 tmpdata\r\n",
        "\u2514\u2500\u2500 w0\r\n",
        "    \u251c\u2500\u2500 indata\r\n",
        "    \u251c\u2500\u2500 outdata\r\n",
        "    \u251c\u2500\u2500 t0\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u251c\u2500\u2500 t1\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u2514\u2500\u2500 tmpdata\r\n",
        "\r\n",
        "15 directories, 7 files\r\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`__AbinitFlow__.pickle` is the pickle database used to store the `AbinitFlow` we've just created (**don't touch it**). \n",
      "`work_0` is the directory containing the input files of the first workflow (well, we have only one workflow in our example).\n",
      "`task_0` and `task_1` contains the input files need to run the SCF and the NSC run, respectively.\n",
      "You might have noticed that each `task_*` directory present the same structure:\n",
      "    \n",
      "   * run.abi: ABINIT input file\n",
      "   * run.files: ABINIT files file\n",
      "   * job.sh: Submission script\n",
      "   * outdata: Directory containing output data files\n",
      "   * indata: Directory containing input data files \n",
      "   * tmpdata: Directory with temporary files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we only need to run our calculations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.make_scheduler().start()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[Tue Feb 10 01:04:19 2015] Number of launches: 1\n",
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=False\n",
        "\n",
        "+------+-------------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "| Task |    Status   |     Queue     | MPI|OMP|Memproc[Gb] | Err|Warn|Comm |  Class   | Restart | Node_ID |\n",
        "+------+-------------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "|  t0  |  \u001b[34mSubmitted\u001b[0m  | 584@localhost |       2|1|2.0       |     0|0|0     | ScfTask  |    0    |  58602  |\n",
        "|  t1  | Initialized\u001b[0m |      None     |       1|1|2.0       |    NA|NA|NA   | NscfTask |    0    |  58603  |\n",
        "+------+-------------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "\n",
        "[Tue Feb 10 01:04:24 2015] Number of launches: 1\n",
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=False\n",
        "\n",
        "+------+-----------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "| Task |   Status  |     Queue     | MPI|OMP|Memproc[Gb] | Err|Warn|Comm |  Class   | Restart | Node_ID |\n",
        "+------+-----------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "|  t0  | \u001b[32mCompleted\u001b[0m | 584@localhost |       2|1|2.0       |     0|1|0     | ScfTask  |    0    |  58602  |\n",
        "|  t1  | \u001b[34mSubmitted\u001b[0m | 593@localhost |       2|1|2.0       |     0|0|0     | NscfTask |    0    |  58603  |\n",
        "+------+-----------+---------------+---------------------+---------------+----------+---------+---------+\n",
        "\n",
        "\n",
        "Work #0: <BandStructureWork, workdir=../../../../../../../../../tmp/hello_bands/w0>, Finalized=\u001b[32mTrue\u001b[0m\n",
        "\n",
        "all_ok reached\n",
        "Submitted on Tue Feb 10 01:04:18 2015\n",
        "Completed on Tue Feb 10 01:04:28 2015\n",
        "Elapsed time 0:00:10.076272\n",
        "Flow completed successfully\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TaskManager"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous sections, we have discussed how to define, build and run a `Flow`, but there is a very important point that we haven't discussed yet.\n",
      "It should be stressed, indeed, that `AbiPy` is only driving and monitoring the `Flow` while the actual calculation is delegated to ABINIT (a Fortran program that is usually executed in parallel on multiple CPUs that communicate via the network by means of the MPI protocol).\n",
      "Besides CPUs and memory must be reserved in advance by sending a request to the resource manager installed on the clusters (SLURM, PBS, etc)\n",
      "\n",
      "`AbiPy` gets all the information needed to submit the different `Tasks` from a configuration file, `manager.yml`, that is usually located in the directory `~/.abinit/abipy/`.\n",
      "For a brief introduction to the YAML format, please consult blablabla."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ~/.abinit/abipy/manager.yml"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "qadapters:\r\n",
        "    - priority: 1\r\n",
        "      queue:\r\n",
        "        qtype: shell\r\n",
        "        qname: localhost\r\n",
        "      job:\r\n",
        "        mpi_runner: mpirun\r\n",
        "        pre_run: \"source ~/env.sh\"\r\n",
        "        #pre_run: \r\n",
        "        #  - \". ~/Coding/Abinit/bzr_archives/env.sh\"\r\n",
        "        #shell_env:\r\n",
        "        #   PATH: /Users/gmatteo/Coding/Abinit/bzr_archives/7113/gmatteo-private/fftw3/src/98_main/:$PATH\r\n",
        "      limits:\r\n",
        "        timelimit: 1:00:00\r\n",
        "        min_cores: 1\r\n",
        "        max_cores: 2\r\n",
        "        # Optional\r\n",
        "        #condition: {\"$eq\": {omp_threads: 2}} \r\n",
        "      hardware:\r\n",
        "         num_nodes: 1\r\n",
        "         sockets_per_node: 1\r\n",
        "         cores_per_socket: 2\r\n",
        "         mem_per_node: 4 Gb\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before running our first calculation with `abipy`, we have to provide two simple configurations files in YAML format.\n",
      "These two files are located in the directory ~/.abinit/abipy and are named `manager.yml` and `scheduler.yml`.\n",
      "The first file provides the information needed to submit and run the abinit jobs on your machine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, we are running abinit in a shell subprocess (qtype: shell) with mpirun. \n",
      "The `autoparal` section defines the variables governing the automated parallelization of the calculation (for more info see the Abinit documentation). \n",
      "`max_npus` defines the *maximum* number of MPI processes that can be used for each run.\n",
      "Before submitting/runnning the calculation, Abipy will perform a fake run on the fron-end to get the possible parallel configurations with ncpus <= max_ncpus. On the basis of these results, abipy will select the one \n",
      "with the highest speedup, and it will also change the input file by setting all the variables that are needed to enable the selected parallel algorithm (very useful, especially for calculations done with `paral_kgb=1`)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scheduler"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other configuration file is named `scheduler.yml` and defines the parameters \n",
      "for the scheduler that will run/submit our jobs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ~/.abinit/abipy/scheduler.yml"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# number of weeks to wait.\r\n",
        "#weeks:\r\n",
        "# number of days to wait.\r\n",
        "#days:\r\n",
        "# number of hours to wait.\r\n",
        "#hours:\r\n",
        "# number of minutes to wait.\r\n",
        "#minutes:\r\n",
        "# number of seconds to wait.\r\n",
        "seconds: 5\r\n",
        "max_nlaunches: 2\r\n",
        "# when to first execute the job and start the counter (default is after the given interval).\r\n",
        "#start_date:\r\n",
        "# Send mail to the specified address (accepts string or list of strings).\r\n",
        "#mailto: matteo.giantomassi@uclouvain.be\r\n",
        "\r\n",
        "# The scheduler will shutdown when the number of python exceptions is > MAX_NUM_PYEXCS\r\n",
        "#MAX_NUM_PYEXCS = 2\r\n",
        "# The scheduler will shutdown when the number of Abinit errors is > MAX_NUM_ABIERRS\r\n",
        "#MAX_NUM_ABIERRS = 0\r\n",
        "# The scheduler will shutdows when the number of tasks launched is > SAFETY_RATIO * tot_num_tasks.\r\n",
        "#SAFETY_RATIO = 3\r\n",
        "#REMINDME_S: 20\r\n",
        "#MAX_TIME_S:\r\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Why Abipy does not support datasets?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multiple datasets are handy if one wants to perform simple calculations such as convergence studies. \n",
      "Unfortunately multiple datasets do not represent the best approach when we are dealing with expensive runs involving different steps.\n",
      "Each step, indeed, may use a different MPI algorithm and it is very difficult to find an optimal number of processors that will lead to a good paralell efficiency in each step. \n",
      "For example, a conjugate-gradient GS calculation parallelized over $k$-points is not efficient when the number of processors is greater than `nkpt*nsppol` whereas a $GW$ calculation (done with the same mesh of $k$-points) is only limited by the number of bands. \n",
      "Moreover a run with multiple datasets might be killed by the resource manager if the job exceeds the maximum wall time and the user is therefore forced to restart the interrupted run. \n",
      "`Abipy` tries to facilitate the execution of calculations that involve several steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}