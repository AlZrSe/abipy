{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tasks, Workflows and Flows"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multiple datasets are handy if one wants to perform simple calculations such as convergence studies. \n",
      "Unfortunately multiple datasets do not represent the best approach when we are dealing with expensive calculations involving different steps.\n",
      "Each step, indeed, might use a different MPI algorithm and it is very difficult to find an optimal number of processors that will lead to a good paralell efficiency in each step. \n",
      "For example, a GS calculation parallelized over k-points is not efficient when the numer of processors is greater than `nkpt*nsppol` whereas a $GW$ calculation (done with the same parameters) is only limited by the number of bands. \n",
      "Moreover a run with multiple datasets might be killed by the resource manager if the job exceeds the maximum wall time and the user is therefore forced to restart the interrupted run. \n",
      "`Abipy` tries to facilitate the execution of calculations that involve several steps...  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by creating a functions that will produce two input files. \n",
      "The first input is a standard self-consistent ground-state calculation. \n",
      "In the example, we use `ndtset=2` simply because this trick allows us to reduce the number of calls we have to perform to initialize the variables of the calculations. For example we set the structure only once by calling `inp.set_structure_from_file` so that both dataset1 and dataset2 will use the same crystalline parameters.\n",
      "The function `split_datasets` returns **two inputs files** that can be run separately. \n",
      "The second dataset uses the density produced in the first run to perform a non self-consistent band structure calculation.\n",
      "**Note** that we don't have to use `getden2 = -1` in the second dataset since `Abipy` knows how to connect the two steps."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A simple exampe: an `AbinitFlow` for the computation of the Silicon band structure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division, print_function\n",
      "\n",
      "import os\n",
      "from abipy import abilab\n",
      "import abipy.data as data\n",
      "\n",
      "def make_scf_nscf_inputs():\n",
      "    inp = abilab.AbiInput(pseudos=data.pseudos(\"14si.pspnc\"), ndtset=2)\n",
      "    inp.set_structure_from_file(data.cif_file(\"si.cif\"))\n",
      "\n",
      "    # Global variables\n",
      "    global_vars = dict(ecut=6,\n",
      "                       nband=8,\n",
      "                    )\n",
      "\n",
      "    inp.set_variables(**global_vars)\n",
      "\n",
      "    # Dataset 1 (GS run)\n",
      "    inp[1].set_kmesh(ngkpt=[8,8,8], shiftk=[0,0,0])\n",
      "    inp[1].set_variables(tolvrs=1e-6)\n",
      "\n",
      "    # Dataset 2 (NSCF run)\n",
      "    kptbounds = [\n",
      "        [0.5, 0.0, 0.0], # L point\n",
      "        [0.0, 0.0, 0.0], # Gamma point\n",
      "        [0.0, 0.5, 0.5], # X point\n",
      "    ]\n",
      "\n",
      "    inp[2].set_kpath(ndivsm=6, kptbounds=kptbounds)\n",
      "    inp[2].set_variables(tolwfr=1e-12)\n",
      "    \n",
      "    # Generate two input files for the GS and the NSCF run\n",
      "    scf_input, nscf_input = inp.split_datasets()\n",
      "    return scf_input, nscf_input"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/gmatteo/Coding/abipy/abipy/data/__init__.py:65: UserWarning: Found duplicated basename out_EIG.nc\n",
        "Stored: /Users/gmatteo/Coding/abipy/abipy/data/runs/DELTAFACTOR/work_0/task_0/outdata/out_EIG.nc, new /Users/gmatteo/Coding/abipy/abipy/data/runs/DELTAFACTOR/work_0/task_1/outdata/out_EIG.nc\n",
        "\n",
        "  warnings.warn(err_msg)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we define a helper function that constructs our `AbinitFlow`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bands_flow(workdir):\n",
      "    # Call our function to get the two inputs.\n",
      "    scf_input, nscf_input = make_scf_nscf_inputs()\n",
      "    \n",
      "    # Read the TaskManager from the configuration file \"taskmanager.yml\"\n",
      "    manager = abilab.TaskManager.from_user_config()\n",
      "    \n",
      "    # Call the factory function for band structure flows provided by abilab   \n",
      "    flow = abilab.bandstructure_flow(workdir, manager, scf_input, nscf_input)\n",
      "    return flow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we can construct our band structure flow by just passing to `bands_flow` the name of the directory where we want to produce the results: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow = bands_flow(workdir=\"/tmp/hello_bands\")\n",
      "flow.show_status()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "Workflow #0: <BandStructureWorkflow, workdir=../../../../../../../tmp/hello_bands/work_0>, Finalized=False\n",
        "\n",
        "Task          Status  Queue_id  Errors  Warnings  Comments  MPI  OMP  num_restarts  max_restarts  Task Class\n",
        "task_0   Initialized      None     N/A       N/A       N/A    1    1             0            -1     ScfTask\n",
        "task_1   Initialized      None     N/A       N/A       N/A    1    1             0            -1    NscfTask\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.show_dependencies()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Task #   0  1\n",
        "0            \n",
        "1        ^   \n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The flow is still in memory and no file has been produced. In order to build the workflow, we have to use "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flow.build_and_pickle_dump()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!tree /tmp/hello_bands"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/tmp/hello_bands\r\n",
        "\u251c\u2500\u2500 __AbinitFlow__.pickle\r\n",
        "\u251c\u2500\u2500 indata\r\n",
        "\u251c\u2500\u2500 outdata\r\n",
        "\u251c\u2500\u2500 tmpdata\r\n",
        "\u2514\u2500\u2500 work_0\r\n",
        "    \u251c\u2500\u2500 indata\r\n",
        "    \u251c\u2500\u2500 outdata\r\n",
        "    \u251c\u2500\u2500 task_0\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u251c\u2500\u2500 task_1\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 indata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 job.sh\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outdata\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.abi\r\n",
        "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 run.files\r\n",
        "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tmpdata\r\n",
        "    \u2514\u2500\u2500 tmpdata\r\n",
        "\r\n",
        "15 directories, 7 files\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`__AbinitFlow__.pickle` is the pickle database used to store the `AbinitFlow` we've just created (**don't touch it**). \n",
      "`work_0` is the directory containing the input files of the first workflow (well we have only one workflow in our example).\n",
      "`task_0` and `task_1` contains the input files need to run the SCF and the NSC run, respectively.\n",
      "You might have noticed that each `task_*` directory present the same arrangement:\n",
      "    \n",
      "      - run.abi: Abinit input file\n",
      "      - run.files: Abinit files file\n",
      "      - job.sh: Submission script\n",
      "      - outdata: Directory containing output data files\n",
      "      - indata: Directory containing input data files \n",
      "      - tmpdata: Directory with temporary files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we only need to run our calculations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from pymatgen.io.abinitio.launcher import PyFlowsScheduler\n",
      "#sched = PyFlowsScheduler(seconds=30)\n",
      "#sched.add_flow(flow)\n",
      "#sched.start()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "================================================================================\n",
        "Workflow #0: <BandStructureWorkflow, workdir=../../../../../../../tmp/hello_bands/work_0>, Finalized=True\n",
        "\n",
        "Task        Status  Queue_id  Errors  Warnings  Comments  MPI  OMP  num_restarts  max_restarts  Task Class\n",
        "task_0   Completed      1827       0         2         0    2    1             0            -1     ScfTask\n",
        "task_1   Completed      1837       0         2         0    2    1             0            -1    NscfTask\n",
        "Number of Tasks launched:  0\n",
        "==============================================================================================\n",
        "=====All tasks in the workflow(s) have reached S_OK. Will shutdown the scheduler and exit=====\n",
        "==============================================================================================\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}