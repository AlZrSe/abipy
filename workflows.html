

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Workflows &mdash; abipy 0.1.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="abipy 0.1.0 documentation" href="index.html"/>
        <link rel="next" title="API documentation" href="api/index.html"/>
        <link rel="prev" title="plot example code: plot_spectral_functions.py" href="examples/plot/plot_spectral_functions.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> abipy
          

          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="features.html">Feature Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Getting AbiPy</a></li>
<li class="toctree-l1"><a class="reference internal" href="whats_new.html">What&#8217;s new in abipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="scripts.html">Command line tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/plot/index.html">plot Examples</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Workflows</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#how-to-configure-the-taskmanager">How to configure the TaskManager</a></li>
<li class="toctree-l2"><a class="reference internal" href="#taskmanager-for-a-personal-computer">TaskManager for a personal computer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-configure-the-scheduler">How to configure the scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuring-abipy-on-a-cluster">Configuring AbiPy on a cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#event-handlers">Event handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#taskpolicy">TaskPolicy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/index.html">API documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="devel/index.html">The AbiPy Developers&#8217; Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">abipy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Workflows</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/workflows.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="workflows">
<span id="id1"></span><h1>Workflows<a class="headerlink" href="#workflows" title="Permalink to this headline">¶</a></h1>
<p>Besides post-processing tools and a programmatic interface to generate input files,
AbiPy also provides a pythonic API to execute small Abinit tasks or submit calculations on supercomputing centers.
This section discusses how to create the configuration files required to interface AbiPy with Abinit.</p>
<p>We assume that Abinit is already available on your machine and that you know how to configure
your environment so that the operating system can load and execute the code.
In other words, we assume that you know how to set the <code class="docutils literal"><span class="pre">$PATH</span></code> and <code class="docutils literal"><span class="pre">$LD_LIBRARY_PATH</span></code> (<code class="docutils literal"><span class="pre">$DYLD_LIBRARY_PATH</span></code> on Mac)
environment variables, load modules with <code class="docutils literal"><span class="pre">module</span> <span class="pre">load</span></code>, run MPI applications with <code class="docutils literal"><span class="pre">mpirun</span></code>, etc.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Please make sure that you can execute Abinit interactively with simple input files and
that the code works as expected before proceeding with the rest of the tutorial.
It&#8217;s also a very good idea to run the Abinit test suite with the <a class="reference external" href="https://asciinema.org/a/40324">runtest.py script</a>
before running production calculations.</p>
</div>
<div class="section" id="how-to-configure-the-taskmanager">
<h2>How to configure the TaskManager<a class="headerlink" href="#how-to-configure-the-taskmanager" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal"><span class="pre">TaskManager</span></code> is responsible for task submission
(creation of the submission script, initialization of the environment) as well as for the
optimization of the parallel algorithms
(number of MPI processes, number of OpenMP threads, automatic parallelization with Abinit <code class="docutils literal"><span class="pre">autoparal</span></code> feature).</p>
<p>AbiPy knows how to run or submit the code with the correct environment and the appropriate syntax
thanks to the options specified in the <code class="docutils literal"><span class="pre">manager.yml</span></code> configuration file.
The file is written in <a class="reference external" href="https://en.wikipedia.org/wiki/YAML">YAML</a>,
a human-readable data serialization language commonly used for configuration files
(a good introduction to the YAML syntax can be found <a class="reference external" href="http://yaml.org/spec/1.1/#id857168">here</a>.
See also this <a class="reference external" href="http://www.yaml.org/refcard.html">reference card</a>)</p>
<p>By default, AbiPy looks for a <code class="docutils literal"><span class="pre">manager.yml</span></code> file in the current working directory i.e.
the directory in which you execute your script and then inside <code class="docutils literal"><span class="pre">$HOME/.abinit/abipy</span></code>.
If no file is found, the code aborts immediately.</p>
<p>At the time of writing (Mar 09, 2017), AbiPy provides adapters (<code class="docutils literal"><span class="pre">qadapters</span></code> in AbiPy jargon)
for the following resource managers:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">bluegene</span></code></li>
<li><code class="docutils literal"><span class="pre">moab</span></code></li>
<li><code class="docutils literal"><span class="pre">pbspro</span></code></li>
<li><code class="docutils literal"><span class="pre">sge</span></code></li>
<li><code class="docutils literal"><span class="pre">shell</span></code></li>
<li><code class="docutils literal"><span class="pre">slurm</span></code></li>
<li><code class="docutils literal"><span class="pre">torque</span></code></li>
</ul>
</div></blockquote>
<p>Configuration files for typical cases are available inside <code class="docutils literal"><span class="pre">~abipy/data/managers</span></code>.</p>
<p>We first discuss how to configure AbiPy on a personal computer and then we look at the more
complicated case in which the calculation must be submitted to a queue.</p>
</div>
<div class="section" id="taskmanager-for-a-personal-computer">
<h2>TaskManager for a personal computer<a class="headerlink" href="#taskmanager-for-a-personal-computer" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s start from the simplest case i.e. a personal computer in which we can execute
applications directly from the shell (<code class="docutils literal"><span class="pre">qtype:</span> <span class="pre">shell</span></code>).
In this case, the configuration file is relatively easy because we can execute Abinit
directly without having to generate and submit a script to the resource manager.
In its simplest form, the <code class="docutils literal"><span class="pre">manager.yml</span></code> file consists of a list of <code class="docutils literal"><span class="pre">qadapters</span></code>:</p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">qadapters</span><span class="p p-Indicator">:</span>
    <span class="p p-Indicator">-</span>  <span class="c1"># qadapter_0</span>

    <span class="p p-Indicator">-</span>  <span class="c1"># qadapter_1</span>
</pre></div>
</div>
<p>Each item in the <code class="docutils literal"><span class="pre">qadapters</span></code> list is essentially a YAML dictionary with the following sub-dictionaries:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">queue</span></code></dt>
<dd>Dictionary with the name of the queue and optional parameters
used to build and customize the header of the submission script.</dd>
<dt><code class="docutils literal"><span class="pre">job</span></code></dt>
<dd>Dictionary with the options used to prepare the environment before submitting the job.</dd>
<dt><code class="docutils literal"><span class="pre">limits</span></code></dt>
<dd>Dictionary with the constraints that must be fulfilled in order to run with this <code class="docutils literal"><span class="pre">qadapter</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">hardware</span></code></dt>
<dd>Dictionary with information on the hardware available on this particular queue.
Used by Abinit <code class="docutils literal"><span class="pre">autoparal</span></code> to optimize parallel execution.</dd>
</dl>
<p>The <code class="docutils literal"><span class="pre">qadapter</span></code> is therefore responsible for all interactions with a specific
queue management system (shell, Slurm, PBS, etc), including handling all details
of queue script format as well as queue submission and management.</p>
<p>The configuration file I use on my laptop to run jobs via the shell is:</p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">qadapters</span><span class="p p-Indicator">:</span> <span class="c1"># List of `qadapters` objects  (just one in this simplified example)</span>

    <span class="p p-Indicator">-</span>  <span class="l l-Scalar l-Scalar-Plain">priority</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
       <span class="l l-Scalar l-Scalar-Plain">queue</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">qtype</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">shell</span>        <span class="c1"># &quot;Submit&quot; jobs via the shell.</span>
        <span class="l l-Scalar l-Scalar-Plain">qname</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">localhost</span>    <span class="c1"># &quot;Submit&quot; to the localhost queue (it&#39;s a fake queue in this case)</span>

    <span class=" -Error">  </span><span class="l l-Scalar l-Scalar-Plain">job</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">pre_run</span><span class="p p-Indicator">:</span> <span class="s">&quot;export</span><span class="nv"> </span><span class="s">PATH=$HOME/git_repos/abinit/build_gcc/src/98_main:$PATH&quot;</span>
        <span class="l l-Scalar l-Scalar-Plain">mpi_runner</span><span class="p p-Indicator">:</span> <span class="s">&quot;mpirun&quot;</span>

      <span class="l l-Scalar l-Scalar-Plain">limits</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">timelimit</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1:00:00</span>   <span class="c1">#  Time-limit for each task.</span>
        <span class="l l-Scalar l-Scalar-Plain">max_cores</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>         <span class="c1">#  Max number of cores that can be used by a single task.</span>

      <span class="c1"># Hardware specification</span>
      <span class="l l-Scalar l-Scalar-Plain">hardware</span><span class="p p-Indicator">:</span>
         <span class="l l-Scalar l-Scalar-Plain">num_nodes</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
         <span class="l l-Scalar l-Scalar-Plain">sockets_per_node</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
         <span class="l l-Scalar l-Scalar-Plain">cores_per_socket</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
         <span class="l l-Scalar l-Scalar-Plain">mem_per_node</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">4 Gb</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">job</span></code> section is the most critical one, in particular the <code class="docutils literal"><span class="pre">pre_run</span></code> option
that will be executed by the shell script before invoking Abinit.
On my laptop, I don&#8217;t install Abinit (developers never install the code they develop)
so I have to prepend the directory where the Abinit executables are located to my original <code class="docutils literal"><span class="pre">$PATH</span></code> variable.
Change <code class="docutils literal"><span class="pre">pre_run</span></code> according to your Abinit installation and make sure that <code class="docutils literal"><span class="pre">mpirun</span></code> is also in <code class="docutils literal"><span class="pre">$PATH</span></code>.
If you don&#8217;t use a parallel version of Abinit, just set <code class="docutils literal"><span class="pre">mpi_runner:</span> <span class="pre">null</span></code>
(<code class="docutils literal"><span class="pre">null</span></code> is the YAML version of the Python <code class="docutils literal"><span class="pre">None</span></code>).</p>
<p>Copy this example and change the entries in the <code class="docutils literal"><span class="pre">hardware</span></code> and the <code class="docutils literal"><span class="pre">limits</span></code> section according to
your machine, in particular make sure that <code class="docutils literal"><span class="pre">max_cores</span></code> is not greater than the number of physical cores
available on personal computer.
Save the file in the current working directory and run the <code class="docutils literal"><span class="pre">abicheck.py</span></code> script provided by AbiPy.
If everything is configured properly, you should see something like this in the terminal.</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>$ abicheck.py --no-colors
AbiPy Manager:
[Qadapter 0]
ShellAdapter:localhost
Hardware:
   num_nodes: 1, sockets_per_node: 1, cores_per_socket: 2, mem_per_node 4096,
Qadapter selected: 0

Abinitbuild:
Abinit Build Information:
    Abinit version: 8.3.1
    MPI: True, MPI-IO: True, OpenMP: False
    Netcdf: True, ETSF-IO: False

Abipy Scheduler:
PyFlowScheduler, Pid: 3599
Scheduler options: {&#39;seconds&#39;: 10, &#39;hours&#39;: 0, &#39;weeks&#39;: 0, &#39;minutes&#39;: 0, &#39;days&#39;: 0}

Installed packages:
Package      Version
-----------  -----------------------
numpy        1.10.4
scipy        0.17.0
netCDF4      1.2.4
apscheduler  2.1.0
pydispatch   2.0.5
yaml         3.11
pymatgen     4.6.2
matplotlib   1.5.1 (backend: Qt4Agg)


Abipy requirements are properly configured
</pre></div>
</div>
<p>This message tells us that everything is in place and we can finally run our first calculation.</p>
<p>The directory <code class="docutils literal"><span class="pre">~abipy/data/runs</span></code> contains python scripts to generate workflows for typical ab-initio calculations.
Here we focus on the configuration of the manager and the execution of the flow so we don&#8217;t to explain how to
generate input files and create Flow objects in python.
This topic is covered in more detail in our collection of <a class="reference external" href="http://nbviewer.ipython.org/github/abinit/abipy/blob/master/abipy/examples/notebooks/index.ipynb">jupyter notebooks</a></p>
<p>Let&#8217;s start from the simplest example i.e. the <code class="docutils literal"><span class="pre">run_si_ebands.py</span></code> script that generates
a flow to compute the band structure of silicon at the Kohn-Sham level
(GS calculation to get the density followed by a NSCF run along a k-path in the first Brillouin zone).</p>
<p>Cd to <code class="docutils literal"><span class="pre">~abipy/data/runs</span></code> and execute <code class="docutils literal"><span class="pre">run_si_ebands.py</span></code> to generate the flow:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ cd ~abipy/data/runs
$ ./run_si_ebands.py
</pre></div>
</div>
<p>At this point, you should have a directory <code class="docutils literal"><span class="pre">flow_si_ebands</span></code> with the following structure:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> tree flow_si_ebands/

<span class="go">flow_si_ebands/</span>
<span class="go">├── __AbinitFlow__.pickle</span>
<span class="go">├── indata</span>
<span class="go">├── outdata</span>
<span class="go">├── tmpdata</span>
<span class="go">└── w0</span>
<span class="go">    ├── indata</span>
<span class="go">    ├── outdata</span>
<span class="go">    ├── t0</span>
<span class="go">    │   ├── indata</span>
<span class="go">    │   ├── job.sh</span>
<span class="go">    │   ├── outdata</span>
<span class="go">    │   ├── run.abi</span>
<span class="go">    │   ├── run.files</span>
<span class="go">    │   └── tmpdata</span>
<span class="go">    ├── t1</span>
<span class="go">    │   ├── indata</span>
<span class="go">    │   ├── job.sh</span>
<span class="go">    │   ├── outdata</span>
<span class="go">    │   ├── run.abi</span>
<span class="go">    │   ├── run.files</span>
<span class="go">    │   └── tmpdata</span>
<span class="go">    └── tmpdata</span>

<span class="go">15 directories, 7 files</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">w0/</span></code> is the directory containing the input files of the first workflow (well, we have only one workflow in our example).
<code class="docutils literal"><span class="pre">w0/t0/</span></code> and <code class="docutils literal"><span class="pre">w0/t1/</span></code> contain the input files need to run the SCF and the NSC run, respectively.</p>
<p>You might have noticed that each <cite>Task</cite> directory (<code class="docutils literal"><span class="pre">w0/t0</span></code>, <code class="docutils literal"><span class="pre">w0/t1</span></code>) presents the same structure:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">run.abi</span></code>: Abinit input file.</li>
<li><code class="docutils literal"><span class="pre">run.files</span></code>: Abinit files file.</li>
<li><code class="docutils literal"><span class="pre">job.sh</span></code>: Submission/shell script.</li>
<li><code class="docutils literal"><span class="pre">outdata</span></code>: Directory with output data files.</li>
<li><code class="docutils literal"><span class="pre">indata</span></code>: Directory with input data files.</li>
<li><code class="docutils literal"><span class="pre">tmpdata</span></code>: Directory with temporary files.</li>
</ul>
</div></blockquote>
<div class="admonition danger">
<p class="first admonition-title">Danger</p>
<p class="last"><code class="docutils literal"><span class="pre">__AbinitFlow__.pickle</span></code> is the pickle file used to save the status of the <cite>Flow</cite>. Don&#8217;t touch it!</p>
</div>
<p>The <code class="docutils literal"><span class="pre">job.sh</span></code> script has been generated using the information provided by <code class="docutils literal"><span class="pre">manager.yml</span></code>.
In this case it&#8217;s a simple shell script that executes the code directly as we are using <code class="docutils literal"><span class="pre">qtype:</span> <span class="pre">shell</span></code>.
The script will be more complicated when we start to submit jobs on a cluster with a resource manager.</p>
<p>We usually interact with the AbiPy flow via the <code class="docutils literal"><span class="pre">abirun.py</span></code> script.
The script uses the syntax:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR command [options]
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">FLOWDIR</span></code> is the directory containing the flow and <code class="docutils literal"><span class="pre">command</span></code> defines the action to perform
(use <code class="docutils literal"><span class="pre">abirun.py</span> <span class="pre">--help</span></code> to get the list of possible commands).</p>
<p><code class="docutils literal"><span class="pre">abirun.py</span></code> reconstruct the python Flow from the pickle file <code class="docutils literal"><span class="pre">__AbinitFlow__.pickle</span></code> located in <code class="docutils literal"><span class="pre">FLOWDIR</span></code>
and invokes the methods of the object depending on the options specified by the user on the command line.
Let&#8217;s start to play with our flow.</p>
<p>Use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands status
</pre></div>
</div>
<p>to have a summary with the status of the different tasks and:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands deps
</pre></div>
</div>
<p>to print the interconnection among the tasks in textual format.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">&lt;ScfTask, node_id=75244, workdir=flow_si_ebands/w0/t0&gt;</span>

<span class="go">&lt;NscfTask, node_id=75245, workdir=flow_si_ebands/w0/t1&gt;</span>
<span class="go">  +--&lt;ScfTask, node_id=75244, workdir=flow_si_ebands/w0/t0&gt;</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Alternatively one can use <code class="docutils literal"><span class="pre">abirun.py</span> <span class="pre">flow_si_ebands</span> <span class="pre">networkx</span></code>
to visualize the connections with the <code class="docutils literal"><span class="pre">networkx</span></code> package.</p>
</div>
<p>In this case, we have a flow with two tasks and the second task (<code class="docutils literal"><span class="pre">w0/t1</span></code>)
depends on the <code class="docutils literal"><span class="pre">ScfTask</span></code>, more specifically on the density file produced by it.
This means that the second task cannot be executed/submitted until we have completed the first task.
<code class="docutils literal"><span class="pre">abirun.py</span></code> knows the dependencies of our flow and will use this information to manage the submission/execution
of our tasks.</p>
<p>There are two commands that can be used to launch tasks: <code class="docutils literal"><span class="pre">single</span></code> and <code class="docutils literal"><span class="pre">rapid</span></code>.
The <code class="docutils literal"><span class="pre">single</span></code> command executes the first <code class="docutils literal"><span class="pre">Task</span></code> in the flow that is in the <code class="docutils literal"><span class="pre">READY</span></code> state that is a task
whose dependencies have been fulfilled while <code class="docutils literal"><span class="pre">rapid</span></code> submits <strong>all tasks</strong> of the flow that are in the <code class="docutils literal"><span class="pre">READY</span></code> state.
Let&#8217;s try to run the flow with the <code class="docutils literal"><span class="pre">rapid</span></code> command and see what happens.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> abirun.py flow_si_ebands rapid

<span class="go">Running on gmac2 -- system Darwin -- Python 2.7.12 -- abirun-0.1.0</span>
<span class="go">Number of tasks launched: 1</span>

<span class="go">Work #0: &lt;BandStructureWork, node_id=75239, workdir=flow_si_ebands/w0&gt;, Finalized=False</span>
<span class="go">+--------+-------------+-----------------+--------------+------------+----------+-----------------+----------+-----------+</span>
<span class="go">| Task   | Status      | Queue           | MPI|Omp|Gb   | Warn|Com   | Class    | Sub|Rest|Corr   | Time     |   Node_ID |</span>
<span class="go">+========+=============+=================+==============+============+==========+=================+==========+===========+</span>
<span class="go">| w0_t0  | Submitted   | 71573@localhost | 2|  1|2.0    | 1|  0      | ScfTask  | (1, 0, 0)       | 0:00:00Q |     75240 |</span>
<span class="go">+--------+-------------+-----------------+--------------+------------+----------+-----------------+----------+-----------+</span>
<span class="go">| w0_t1  | Initialized | None            | 1|  1|2.0    | NA|NA      | NscfTask | (0, 0, 0)       | None     |     75241 |</span>
<span class="go">+--------+-------------+-----------------+--------------+------------+----------+-----------------+----------+-----------+</span>
</pre></div>
</div>
<p>What&#8217;s happening here?
The <code class="docutils literal"><span class="pre">rapid</span></code> command tried to execute all tasks that are <code class="docutils literal"><span class="pre">READY</span></code> but since the second task depends
on the first one only the first task gets submitted.
Note that the SCF task (<code class="docutils literal"><span class="pre">w0_t0</span></code>) has been submitted with 2 MPI processes.
Before submitting the task, indeed, AbiPy
invokes Abinit to get all the possible parallel configurations compatible within the limits
specified by the user (e.g. <code class="docutils literal"><span class="pre">max_cores</span></code>), select an &#8220;optimal&#8221; configuration according
to some policy and then submit the task with the optimized parameters.
At this point, there&#8217;s no other task that can be executed, the script exits
and we have to wait for the SCF task before running the second part of the flow.</p>
<p>At each iteration, <code class="docutils literal"><span class="pre">abirun.py</span></code> prints a table with the status of the different tasks.
The meaning of the columns is as follows:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">Queue</span></code></dt>
<dd>String in the form <cite>JobID &#64; QueueName</cite> where JobID is the process identifier if we are using the shell
or the job ID assigned by the resource manager (e.g. slurm) if we are submitting to a queue.</dd>
<dt><code class="docutils literal"><span class="pre">MPI</span></code></dt>
<dd>Number of MPI processes used. This value is obtained automatically by calling Abinit in <code class="docutils literal"><span class="pre">autoparal</span> <span class="pre">mode</span></code>,
cannot exceed <code class="docutils literal"><span class="pre">max_ncpus</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">OMP</span></code></dt>
<dd>Number of OpenMP threads.</dd>
<dt><code class="docutils literal"><span class="pre">Gb</span></code></dt>
<dd>Memory requested in Gb. Meaningless when <code class="docutils literal"><span class="pre">qtype:</span> <span class="pre">shell</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">Warn</span></code></dt>
<dd>Number of warning messages found in the log file.</dd>
<dt><code class="docutils literal"><span class="pre">Com</span></code></dt>
<dd>Number of comments found in the log file.</dd>
<dt><code class="docutils literal"><span class="pre">Sub</span></code></dt>
<dd>Number of submissions. It can be &gt; 1 if AbiPy encounters a problem and resubmit the task
with different parameters without performing any operation that can change the physics of the system).</dd>
<dt><code class="docutils literal"><span class="pre">Rest</span></code></dt>
<dd>Number of restarts. AbiPy can restart the job if convergence has not been reached.</dd>
<dt><code class="docutils literal"><span class="pre">Corr</span></code></dt>
<dd>Number of corrections performed by AbiPy to fix runtime errors.
These operations can change the physics of the system.</dd>
<dt><code class="docutils literal"><span class="pre">Time</span></code></dt>
<dd>Time spent in the queue (if string ends with Q) or running time (if string ends with R).</dd>
<dt><code class="docutils literal"><span class="pre">Node_ID</span></code></dt>
<dd>Node identifier used by AbiPy to identify each node of the flow.</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When the submission is done through the shell there&#8217;s almost no difference between
job submission and job execution. The scenario is completely different if you are submitting
jobs to a resource manager because the task will get a priority value and will enter the queue.</p>
</div>
<p>If you execute <code class="docutils literal"><span class="pre">status</span></code> again, you should see that the first task is completed.
We can thus run <code class="docutils literal"><span class="pre">rapid</span></code> again to launch the <code class="docutils literal"><span class="pre">NscfTask</span></code>.
The second task won&#8217;t take long and if you issue <code class="docutils literal"><span class="pre">status</span></code> again, you should see that the entire flow
completed successfully.</p>
<p>To understand what happened in more detail, use the <code class="docutils literal"><span class="pre">history</span></code> command to get
the list of operations performed by AbiPy on each task.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> abirun.py flow_si_ebands <span class="nb">history</span>

<span class="go">==============================================================================================================================</span>
<span class="go">=================================== &lt;ScfTask, node_id=75244, workdir=flow_si_ebands/w0/t0&gt; ===================================</span>
<span class="go">==============================================================================================================================</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Status changed to Ready. msg: Status set to Ready</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Setting input variables: {&#39;max_ncpus&#39;: 2, &#39;autoparal&#39;: 1}</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Old values: {&#39;max_ncpus&#39;: None, &#39;autoparal&#39;: None}</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Setting input variables: {&#39;npband&#39;: 1, &#39;bandpp&#39;: 1, &#39;npimage&#39;: 1, &#39;npspinor&#39;: 1, &#39;npfft&#39;: 1, &#39;npkpt&#39;: 2}</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Old values: {&#39;npband&#39;: None, &#39;npfft&#39;: None, &#39;npkpt&#39;: None, &#39;npimage&#39;: None, &#39;npspinor&#39;: None, &#39;bandpp&#39;: None}</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Status changed to Initialized. msg: finished autoparallel run</span>
<span class="go">[Mon Mar  6 21:46:00 2017] Submitted with MPI=2, Omp=1, Memproc=2.0 [Gb] submitted to queue</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Task completed status set to ok based on abiout</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Finalized set to True</span>

<span class="go">=============================================================================================================================</span>
<span class="go">================================== &lt;NscfTask, node_id=75245, workdir=flow_si_ebands/w0/t1&gt; ==================================</span>
<span class="go">=============================================================================================================================</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Status changed to Ready. msg: Status set to Ready</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Adding connecting vars {u&#39;irdden&#39;: 1}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Setting input variables: {u&#39;irdden&#39;: 1}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Old values: {u&#39;irdden&#39;: None}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Setting input variables: {&#39;max_ncpus&#39;: 2, &#39;autoparal&#39;: 1}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Old values: {&#39;max_ncpus&#39;: None, &#39;autoparal&#39;: None}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Setting input variables: {&#39;npband&#39;: 1, &#39;bandpp&#39;: 1, &#39;npimage&#39;: 1, &#39;npspinor&#39;: 1, &#39;npfft&#39;: 1, &#39;npkpt&#39;: 2}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Old values: {&#39;npband&#39;: None, &#39;npfft&#39;: None, &#39;npkpt&#39;: None, &#39;npimage&#39;: None, &#39;npspinor&#39;: None, &#39;bandpp&#39;: None}</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Status changed to Initialized. msg: finished autoparallel run</span>
<span class="go">[Mon Mar  6 21:46:15 2017] Submitted with MPI=2, Omp=1, Memproc=2.0 [Gb] submitted to queue</span>
<span class="go">[Mon Mar  6 21:49:48 2017] Task completed status set to ok based on abiout</span>
<span class="go">[Mon Mar  6 21:49:48 2017] Finalized set to True</span>
</pre></div>
</div>
<p>A closer inspection of the logs reveal that before submitting the first task, python has executed
Abinit in <code class="docutils literal"><span class="pre">autoparal</span></code> mode to get the list of possible parallel configuration and the calculation is then submitted.
At this point, AbiPy starts to look at the output files produced by the task to understand  what&#8217;s happening.
When the first task completes, the second task is automatically changed to <code class="docutils literal"><span class="pre">READY</span></code>,
the <code class="docutils literal"><span class="pre">irdden</span></code> input variable is added to the input file of the second task and a symbolic link to
the <code class="docutils literal"><span class="pre">DEN</span></code> file produced by the first task is created in the <code class="docutils literal"><span class="pre">indata</span></code> directory of the second task.
Another <code class="docutils literal"><span class="pre">autoparal</span> <span class="pre">run</span></code> is executed for the NSCF calculation and the second task is finally submitted.</p>
<p>The command line interface is very flexible and sometimes it&#8217;s the only tool available.
However, there are cases in which we would like to have a global view of what&#8217;s happening.
The command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands notebook
</pre></div>
</div>
<p>generates a <code class="docutils literal"><span class="pre">jupyter</span></code> notebook with pre-defined python code that can be executed
to get a graphical representation of the status of our flow inside a web browser
(requires <code class="docutils literal"><span class="pre">jupyter</span></code>, <code class="docutils literal"><span class="pre">nbformat</span></code> and, obviously, a web browser).</p>
<p>Expert users may want to use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands ipython
</pre></div>
</div>
<p>to open the flow in the <code class="docutils literal"><span class="pre">ipython</span></code> shell to have direct access to the API provided by the flow.</p>
<p>Once <code class="docutils literal"><span class="pre">manager.yml</span></code> is properly configured, it is possible
to use the AbiPy objects to invoke Abinit and perform small but quite useful operations.
For example, one can use the <code class="docutils literal"><span class="pre">AbinitInput</span></code> object to get the list of k-points in the IBZ,
the list of independent DFPT perturbations, the possible parallel configurations reported by <code class="docutils literal"><span class="pre">autoparal</span></code> etc.</p>
<p>This programmatic interface can be used in scripts to facilitate the creation of input files and workflows.
For example, one can call Abinit to get the list of perturbations for each q-point in the IBZ and then
generate automatically all the input files for DFPT calculations (actually this is the approach used to
generated DFPT workflows in the AbiPy factory functions).</p>
<p>Note that <code class="docutils literal"><span class="pre">manager.yml</span></code> is also used to invoke other executables (<code class="docutils literal"><span class="pre">anaddb</span></code>, <code class="docutils literal"><span class="pre">optic</span></code>, <code class="docutils literal"><span class="pre">mrgddb</span></code>, etcetera)
thus creating some sort of interface between the python language and the Fortran executables.
Thanks to this interface, one can perform relatively simple ab-initio calculations directly in AbiPy,
for instance it is possible to open a <code class="docutils literal"><span class="pre">DDB</span></code> file in a jupyter notebook, call <code class="docutils literal"><span class="pre">anaddb</span></code> to compute
the phonon frequencies and plot the DOS and the phonon band structure with <code class="docutils literal"><span class="pre">matplotlib</span></code>.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<blockquote>
<div>$ abirun.py . doc_manager</div></blockquote>
<p class="last">gives the full documentation for the different entries of <code class="docutils literal"><span class="pre">manager.yml</span></code>.</p>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>$ abirun.py . doc_manager

# TaskManager configuration file (YAML Format)

policy:
    # Dictionary with options used to control the execution of the tasks.

qadapters:
    # List of qadapters objects (mandatory)
    -  # qadapter_1
    -  # qadapter_2

db_connector:
    # Connection to MongoDB database (optional)

batch_adapter:
    # Adapter used to submit flows with batch script. (optional)

##########################################
# Individual entries are documented below:
##########################################

policy: 
    autoparal:                # (integer). 0 to disable the autoparal feature (DEFAULT: 1 i.e. autoparal is on)
    condition:                # condition used to filter the autoparal configurations (Mongodb-like syntax).
                              # DEFAULT: empty i.e. ignored.
    vars_condition:           # Condition used to filter the list of ABINIT variables reported by autoparal
                              # (Mongodb-like syntax). DEFAULT: empty i.e. ignored.
    frozen_timeout:           # A job is considered frozen and its status is set to ERROR if no change to
                              # the output file has been done for `frozen_timeout` seconds. Accepts int with seconds or
                              # string in slurm form i.e. days-hours:minutes:seconds. DEFAULT: 1 hour.
    precedence:               # Under development.
    autoparal_priorities:     # Under development.

qadapter: 
# Dictionary with info on the hardware available on this queue.
hardware:
    num_nodes:           # Number of nodes available on this queue (integer, MANDATORY).
    sockets_per_node:    # Number of sockets per node (integer, MANDATORY).
    cores_per_socket:    # Number of cores per socket (integer, MANDATORY).
                         # The total number of cores available on this queue is
                         # `num_nodes * sockets_per_node * cores_per_socket`.

# Dictionary with the options used to prepare the enviroment before submitting the job
job:
    setup:            # List of commands (strings) executed before running (DEFAULT: empty)
    omp_env:          # Dictionary with OpenMP environment variables (DEFAULT: empty i.e. no OpenMP)
    modules:          # List of modules to be imported before running the code (DEFAULT: empty).
                      # NB: Error messages produced by module load are redirected to mods.err
    shell_env:        # Dictionary with shell environment variables.
    mpi_runner:       # MPI runner. Possible values in [mpirun, mpiexec, None]
                      # DEFAULT: None i.e. no mpirunner is used.
    shell_runner:     # Used for running small sequential jobs on the front-end. Set it to None
                      # if mpirun or mpiexec are not available on the fron-end. If not
                      # given, small sequential jobs are executed with `mpi_runner`.
    pre_run:          # List of commands (strings) executed before the run (DEFAULT: empty)
    post_run:         # List of commands (strings) executed after the run (DEFAULT: empty)

# dictionary with the name of the queue and optional parameters
# used to build/customize the header of the submission script.
queue:
    qname:            # Name of the queue (string, MANDATORY)
    qparams:          # Dictionary with values used to generate the header of the job script
                      # See pymatgen.io.abinit.qadapters.py for the list of supported values.

# dictionary with the constraints that must be fulfilled in order to run on this queue.
limits:
    min_cores:         # Minimum number of cores (integer, DEFAULT: 1)
    max_cores:         # Maximum number of cores (integer, MANDATORY). Hard limit to hint_cores:
                       # it&#39;s the limit beyond which the scheduler will not accept the job (MANDATORY).
    hint_cores:        # The limit used in the initial setup of jobs.
                       # Fix_Critical method may increase this number until max_cores is reached
    min_mem_per_proc:  # Minimum memory per MPI process in Mb, units can be specified e.g. 1.4 Gb
                       # (DEFAULT: hardware.mem_per_core)
    max_mem_per_proc:  # Maximum memory per MPI process in Mb, units can be specified e.g. `1.4Gb`
                       # (DEFAULT: hardware.mem_per_node)
    timelimit:         # Initial time-limit. Accepts time according to slurm-syntax i.e:
                       # &quot;days-hours&quot; or &quot;days-hours:minutes&quot; or &quot;days-hours:minutes:seconds&quot; or
                       # &quot;minutes&quot; or &quot;minutes:seconds&quot; or &quot;hours:minutes:seconds&quot;,
    timelimit_hard:    # The hard time-limit for this queue. Same format as timelimit.
                       # Error handlers could try to submit jobs with increased timelimit
                       # up to timelimit_hard. If not specified, timelimit_hard == timelimit
    condition:         # MongoDB-like condition (DEFAULT: empty, i.e. not used)
    allocation:        # String defining the policy used to select the optimal number of CPUs.
                       # possible values are in [&quot;nodes&quot;, &quot;force_nodes&quot;, &quot;shared&quot;]
                       # &quot;nodes&quot; means that we should try to allocate entire nodes if possible.
                       # This is a soft limit, in the sense that the qadapter may use a configuration
                       # that does not fulfill this requirement. In case of failure, it will try to use the
                       # smallest number of nodes compatible with the optimal configuration.
                       # Use `force_nodes` to enfore entire nodes allocation.
                       # `shared` mode does not enforce any constraint (DEFAULT: shared).
    max_num_launches:  # Limit to the number of times a specific task can be restarted (integer, DEFAULT: 5)


qtype supported: [u&#39;bluegene&#39;, u&#39;moab&#39;, u&#39;pbspro&#39;, u&#39;sge&#39;, u&#39;shell&#39;, u&#39;slurm&#39;, u&#39;torque&#39;]
Use `abirun.py . manager slurm` to have the list of qparams for slurm.
</pre></div>
</div>
</div>
<div class="section" id="how-to-configure-the-scheduler">
<h2>How to configure the scheduler<a class="headerlink" href="#how-to-configure-the-scheduler" title="Permalink to this headline">¶</a></h2>
<p>In the previous example, we ran a simple band structure calculation for silicon in a few seconds
on a laptop but one might have more complicated flows requiring hours or even days to complete.
For such cases, the <code class="docutils literal"><span class="pre">single</span></code> and <code class="docutils literal"><span class="pre">rapid</span></code> commands are not handy because we are supposed
to monitor the evolution of the flow and re-run <code class="docutils literal"><span class="pre">abirun.py</span></code> when a new task is <code class="docutils literal"><span class="pre">READY</span></code>.
In these cases, it is much easier to delegate all the repetitive work to a <code class="docutils literal"><span class="pre">python</span> <span class="pre">scheduler</span></code>,
a process that runs in the background, submits tasks automatically and performs the actions
required to complete the flow.</p>
<p>The parameters for the scheduler are declared in the YAML file <code class="docutils literal"><span class="pre">scheduler.yml</span></code>.
Also in this case, AbiPy will look first in the working directory and then inside <code class="docutils literal"><span class="pre">$HOME/.abinit/abipy</span></code>.
Create a <code class="docutils literal"><span class="pre">scheduler.yml</span></code> in the working directory by copying the example below:</p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">seconds</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>   <span class="c1"># number of seconds to wait.</span>
<span class="c1">#minutes: 0  # number of minutes to wait.</span>
<span class="c1">#hours: 0    # number of hours to wait.</span>
</pre></div>
</div>
<p>This file tells the scheduler to wake up every 5 seconds, inspect the status of the tasks
in the flow and perform the actions required for reach completion</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Remember to set the time interval to a reasonable value.
A small value leads to an increase of the submission rate but it also increases the CPU load
and the pressure on the hardware and on the resource manager.
A too large time interval can have a detrimental effect on the throughput, especially
if you are submitting many small jobs.</p>
</div>
<p>At this point, we are ready to run our first calculation with the scheduler.
To make things more interesting, we execute a slightly more complicated flow that computes
the G0W0 corrections to the direct band gap of silicon at the Gamma point.
The flow consists of the following six tasks:</p>
<ul class="simple">
<li>0: Ground state calculation to get the density.</li>
<li>1: NSCF calculation with several empty states.</li>
<li>2: Calculation of the screening using the WFK produced by task 2.</li>
<li>3-4-5: Evaluation of the Self-Energy matrix elements with different values of nband
using the WFK produced by task 2 and the SCR file produced by task 3</li>
</ul>
<p>Generate the flow with:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ ./run_si_g0w0.py
</pre></div>
</div>
<p>and let the scheduler manage the submission with:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_g0w0 scheduler
</pre></div>
</div>
<p>You should see the following output on the terminal</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> abirun.py flow_si_ebands scheduler

<span class="go">Abipy Scheduler:</span>
<span class="go">PyFlowScheduler, Pid: 72038</span>
<span class="go">Scheduler options: {&#39;seconds&#39;: 10, &#39;hours&#39;: 0, &#39;weeks&#39;: 0, &#39;minutes&#39;: 0, &#39;days&#39;: 0}</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">Pid</span></code> is the process identifier of the scheduler (also reported in the ... file)
We will see that the scheduler pid is extremely important when we start to run large flows on clusters.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Note that there must be only one scheduler associated to a given flow.</p>
</div>
<p>As you can easily understand the scheduler brings additional power to the AbiPy flow because
it is possible to automate complicated ab-initio workflows with little effort (write
a script that implements the flow in python and save it to disk, run it with
<code class="docutils literal"><span class="pre">abirun.py</span> <span class="pre">FLOWDIR</span> <span class="pre">scheduler</span></code> and finally use the AbiPy/Pymatgen tools to analyze the final results).
Even complicated convergence studies for G0W0 calculations can be implemented along these lines
as show by this <a class="reference external" href="https://youtu.be/M9C6iqJsvJI">video</a>.
The only problem is that at a certain point our flow will become too big or too computational expensive
that we cannot run it on a personal computer anymore and we have to move to a supercomputing center.
The next section discusses how to configure AbiPy to run on a cluster with a queue management system.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Use <code class="docutils literal"><span class="pre">abirun.py</span> <span class="pre">.</span> <span class="pre">doc_scheduler</span></code> to get the full list of options supported by the scheduler.</p>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>$ abirun.py doc_scheduler
Options that can be specified in scheduler.yml:

            weeks: number of weeks to wait (DEFAULT: 0).
            days: number of days to wait (DEFAULT: 0).
            hours: number of hours to wait (DEFAULT: 0).
            minutes: number of minutes to wait (DEFAULT: 0).
            seconds: number of seconds to wait (DEFAULT: 0).
            mailto: The scheduler will send an email to `mailto` every `remindme_s` seconds.
                (DEFAULT: None i.e. not used).
            verbose: (int) verbosity level. (DEFAULT: 0)
            use_dynamic_manager: &quot;yes&quot; if the :class:`TaskManager` must be re-initialized from
                file before launching the jobs. (DEFAULT: &quot;no&quot;)
            max_njobs_inqueue: Limit on the number of jobs that can be present in the queue. (DEFAULT: 200)
            remindme_s: The scheduler will send an email to the user specified by `mailto` every `remindme_s` seconds.
                (int, DEFAULT: 1 day).
            max_num_pyexcs: The scheduler will exit if the number of python exceptions is &gt; max_num_pyexcs
                (int, DEFAULT: 0)
            max_num_abierrs: The scheduler will exit if the number of errored tasks is &gt; max_num_abierrs
                (int, DEFAULT: 0)
            safety_ratio: The scheduler will exits if the number of jobs launched becomes greater than
               `safety_ratio` * total_number_of_tasks_in_flow. (int, DEFAULT: 5)
            max_nlaunches: Maximum number of tasks launched in a single iteration of the scheduler.
                (DEFAULT: -1 i.e. no limit)
            debug: Debug level. Use 0 for production (int, DEFAULT: 0)
            fix_qcritical: &quot;yes&quot; if the launcher should try to fix QCritical Errors (DEFAULT: &quot;yes&quot;)
            rmflow: If &quot;yes&quot;, the scheduler will remove the flow directory if the calculation
                completed successfully. (DEFAULT: &quot;no&quot;)
            killjobs_if_errors: &quot;yes&quot; if the scheduler should try to kill all the runnnig jobs
                before exiting due to an error. (DEFAULT: &quot;yes&quot;)
</pre></div>
</div>
</div>
<div class="section" id="configuring-abipy-on-a-cluster">
<h2>Configuring AbiPy on a cluster<a class="headerlink" href="#configuring-abipy-on-a-cluster" title="Permalink to this headline">¶</a></h2>
<p>In this section we discuss how to configure the manager to run flows on a cluster.
The configuration depends on specific queue management system (Slurm, PBS, etc) so
we assume that you are already familiar with job submissions and you know the options
that mush be specified in the submission script in order to have your job accepted
and executed by the management system (username, name of the queue, memory ...)</p>
<p>Let&#8217;s assume that your computing center uses Slurm and your jobs must be submitted to the <cite>Oban</cite> partition
A <cite>manager.yml</cite> with a single <cite>qadapter</cite> looks like:</p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="c1"># Resource manager e.g slurm, pbs, shell</span>
<span class="l l-Scalar l-Scalar-Plain">qtype</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">slurm</span>

<span class="c1"># Options passed to the resource manager</span>
<span class="c1"># (the syntax depends on qtype, consult the manual of your resource manager)</span>
<span class="l l-Scalar l-Scalar-Plain">qparams</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">ntasks</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
  <span class="l l-Scalar l-Scalar-Plain">time</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">0:20:00</span>
  <span class="l l-Scalar l-Scalar-Plain">partition</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">Oban</span>

<span class="c1"># List of modules to import before running the calculation</span>
<span class="l l-Scalar l-Scalar-Plain">modules</span><span class="p p-Indicator">:</span>
    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">intel/compilerpro/13.0.1.117</span>
    <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">fftw3/intel/3.3</span>

<span class="l l-Scalar l-Scalar-Plain">mpi_runner</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/path/to/mpirun</span>

<span class="c1"># Shell environment</span>
<span class="l l-Scalar l-Scalar-Plain">shell_env</span><span class="p p-Indicator">:</span>
     <span class="l l-Scalar l-Scalar-Plain">PATH</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/home/user/local/bin/:$PATH</span>
     <span class="l l-Scalar l-Scalar-Plain">LD_LIBRARY_PATH</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/home/user/local/lib:$LD_LIBRARY_PATH</span>

<span class="c1"># Options for the automatic parallelization (Abinit autoparal feature)</span>
<span class="l l-Scalar l-Scalar-Plain">policy</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">autoparal</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="l l-Scalar l-Scalar-Plain">max_ncpus</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
</pre></div>
</div>
<p>Description:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">qtype</span></code></dt>
<dd>string specifying the resource manager. This option tells AbiPy how to generate the submission
script, submit them, kill jobs in the queue and how to interpret the other options passed by the user.</dd>
<dt><code class="docutils literal"><span class="pre">qparams</span></code></dt>
<dd>Dictionary with the parameters passed to the resource manager.
We use the <em>normalized</em> version of the options i.e dashes in the official name of the parameter
are replaced by underscores (for the list of supported options see ...)</dd>
<dt><code class="docutils literal"><span class="pre">modules</span></code></dt>
<dd>List of modules to load.</dd>
<dt><code class="docutils literal"><span class="pre">shell_env</span></code></dt>
<dd>Allows the user to specify or to modify the values of the environment variables.</dd>
<dt><code class="docutils literal"><span class="pre">policy</span></code></dt>
<dd>This section governs the automatic parallelization of the run: in this case AbiPy will use
the <code class="docutils literal"><span class="pre">autoparal</span></code> capabilities of Abinit to determine an optimal configuration with
<strong>maximum</strong> <code class="docutils literal"><span class="pre">max_ncpus</span></code> MPI nodes. Setting <code class="docutils literal"><span class="pre">autoparal</span></code> to 0 disables the automatic parallelization.
Other values of autoparal are not supported*</dd>
</dl>
<p>The complete list of <code class="docutils literal"><span class="pre">qparams</span></code> options supported with Slurm is be obtained with</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>$ abirun.py . doc_manager slurm

# TaskManager configuration file (YAML Format)

policy:
    # Dictionary with options used to control the execution of the tasks.

qadapters:
    # List of qadapters objects (mandatory)
    -  # qadapter_1
    -  # qadapter_2

db_connector:
    # Connection to MongoDB database (optional)

batch_adapter:
    # Adapter used to submit flows with batch script. (optional)

##########################################
# Individual entries are documented below:
##########################################

policy: 
    autoparal:                # (integer). 0 to disable the autoparal feature (DEFAULT: 1 i.e. autoparal is on)
    condition:                # condition used to filter the autoparal configurations (Mongodb-like syntax).
                              # DEFAULT: empty i.e. ignored.
    vars_condition:           # Condition used to filter the list of ABINIT variables reported by autoparal
                              # (Mongodb-like syntax). DEFAULT: empty i.e. ignored.
    frozen_timeout:           # A job is considered frozen and its status is set to ERROR if no change to
                              # the output file has been done for `frozen_timeout` seconds. Accepts int with seconds or
                              # string in slurm form i.e. days-hours:minutes:seconds. DEFAULT: 1 hour.
    precedence:               # Under development.
    autoparal_priorities:     # Under development.

qadapter: 
# Dictionary with info on the hardware available on this queue.
hardware:
    num_nodes:           # Number of nodes available on this queue (integer, MANDATORY).
    sockets_per_node:    # Number of sockets per node (integer, MANDATORY).
    cores_per_socket:    # Number of cores per socket (integer, MANDATORY).
                         # The total number of cores available on this queue is
                         # `num_nodes * sockets_per_node * cores_per_socket`.

# Dictionary with the options used to prepare the enviroment before submitting the job
job:
    setup:            # List of commands (strings) executed before running (DEFAULT: empty)
    omp_env:          # Dictionary with OpenMP environment variables (DEFAULT: empty i.e. no OpenMP)
    modules:          # List of modules to be imported before running the code (DEFAULT: empty).
                      # NB: Error messages produced by module load are redirected to mods.err
    shell_env:        # Dictionary with shell environment variables.
    mpi_runner:       # MPI runner. Possible values in [mpirun, mpiexec, None]
                      # DEFAULT: None i.e. no mpirunner is used.
    shell_runner:     # Used for running small sequential jobs on the front-end. Set it to None
                      # if mpirun or mpiexec are not available on the fron-end. If not
                      # given, small sequential jobs are executed with `mpi_runner`.
    pre_run:          # List of commands (strings) executed before the run (DEFAULT: empty)
    post_run:         # List of commands (strings) executed after the run (DEFAULT: empty)

# dictionary with the name of the queue and optional parameters
# used to build/customize the header of the submission script.
queue:
    qname:            # Name of the queue (string, MANDATORY)
    qparams:          # Dictionary with values used to generate the header of the job script
                      # See pymatgen.io.abinit.qadapters.py for the list of supported values.

# dictionary with the constraints that must be fulfilled in order to run on this queue.
limits:
    min_cores:         # Minimum number of cores (integer, DEFAULT: 1)
    max_cores:         # Maximum number of cores (integer, MANDATORY). Hard limit to hint_cores:
                       # it&#39;s the limit beyond which the scheduler will not accept the job (MANDATORY).
    hint_cores:        # The limit used in the initial setup of jobs.
                       # Fix_Critical method may increase this number until max_cores is reached
    min_mem_per_proc:  # Minimum memory per MPI process in Mb, units can be specified e.g. 1.4 Gb
                       # (DEFAULT: hardware.mem_per_core)
    max_mem_per_proc:  # Maximum memory per MPI process in Mb, units can be specified e.g. `1.4Gb`
                       # (DEFAULT: hardware.mem_per_node)
    timelimit:         # Initial time-limit. Accepts time according to slurm-syntax i.e:
                       # &quot;days-hours&quot; or &quot;days-hours:minutes&quot; or &quot;days-hours:minutes:seconds&quot; or
                       # &quot;minutes&quot; or &quot;minutes:seconds&quot; or &quot;hours:minutes:seconds&quot;,
    timelimit_hard:    # The hard time-limit for this queue. Same format as timelimit.
                       # Error handlers could try to submit jobs with increased timelimit
                       # up to timelimit_hard. If not specified, timelimit_hard == timelimit
    condition:         # MongoDB-like condition (DEFAULT: empty, i.e. not used)
    allocation:        # String defining the policy used to select the optimal number of CPUs.
                       # possible values are in [&quot;nodes&quot;, &quot;force_nodes&quot;, &quot;shared&quot;]
                       # &quot;nodes&quot; means that we should try to allocate entire nodes if possible.
                       # This is a soft limit, in the sense that the qadapter may use a configuration
                       # that does not fulfill this requirement. In case of failure, it will try to use the
                       # smallest number of nodes compatible with the optimal configuration.
                       # Use `force_nodes` to enfore entire nodes allocation.
                       # `shared` mode does not enforce any constraint (DEFAULT: shared).
    max_num_launches:  # Limit to the number of times a specific task can be restarted (integer, DEFAULT: 5)


qtype supported: [u&#39;bluegene&#39;, u&#39;moab&#39;, u&#39;pbspro&#39;, u&#39;sge&#39;, u&#39;shell&#39;, u&#39;slurm&#39;, u&#39;torque&#39;]
Use `abirun.py . manager slurm` to have the list of qparams for slurm.

QPARAMS for slurm
#!/bin/bash

#SBATCH --partition=$${partition}
#SBATCH --job-name=$${job_name}
#SBATCH --nodes=$${nodes}
#SBATCH --total_tasks=$${total_tasks}
#SBATCH --ntasks=$${ntasks}
#SBATCH --ntasks-per-node=$${ntasks_per_node}
#SBATCH --cpus-per-task=$${cpus_per_task}
#####SBATCH --mem=$${mem}
#SBATCH --mem-per-cpu=$${mem_per_cpu}
#SBATCH --hint=$${hint}
#SBATCH --time=$${time}
#SBATCH	--exclude=$${exclude_nodes}
#SBATCH --account=$${account}
#SBATCH --mail-user=$${mail_user}
#SBATCH --mail-type=$${mail_type}
#SBATCH --constraint=$${constraint}
#SBATCH --gres=$${gres}
#SBATCH --requeue=$${requeue}
#SBATCH --nodelist=$${nodelist}
#SBATCH --propagate=$${propagate}
#SBATCH --licenses=$${licenses}
#SBATCH --output=$${_qout_path}
#SBATCH --error=$${_qerr_path}
$${qverbatim}
</pre></div>
</div>
<p>If for some reason you need to cancel all tasks that have been submitted to the resource manager, use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR cancel
</pre></div>
</div>
<p>Note that the script will ask for confirmation before killing all the jobs belonging to the flow.</p>
<p>Once you have a <code class="docutils literal"><span class="pre">manager.yml</span></code> properly configured for your cluster, you can start
to use the scheduler to automate job submission.
Very likely your flows will require hours or even days to complete and, in principle,
you should maintain an active connection to the machine in order to keep your scheduler alive
(if your session expires, all subprocesses launched within your terminal including
the python scheduler  will be automatically killed).
Fortunately there&#8217;s a standard Unix tool called <code class="docutils literal"><span class="pre">nohup</span></code> that comes to our rescue.</p>
<p>For long-running jobs, we strongly suggest to run the scheduler in background with:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ nohup abirun.py FLOWDIR scheduler &gt; sched.log 2&gt; sched.err &amp;
</pre></div>
</div>
<p>This command redirects the stdout and stderr of the process to <code class="docutils literal"><span class="pre">sched.log</span></code> and <code class="docutils literal"><span class="pre">sched.err</span></code>
and kill the active session without killing the scheduler thanks to the <code class="docutils literal"><span class="pre">nohup</span></code> Unix command.
In this case, the PID gives as a handle that can be used to check whether the scheduler
is still running or kill it when we login again.</p>
<p>AbiPy is able to detect if there is a scheduler already attached to the flow and
it will also kill the scheduler</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Please make sure that you can execute Abinit interactively with simple input files and
that the code works as expected before proceeding with the rest of the tutorial.</p>
</div>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h2>
<p>There are two <code class="docutils literal"><span class="pre">abirun.py</span></code> commands that are very handy especially if something goes wrong: <code class="docutils literal"><span class="pre">events</span></code> and <code class="docutils literal"><span class="pre">debug</span></code>.</p>
<p>To print the Abinit events (Warnings, Errors, Comments) found in the log files of the different tasks use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR events
</pre></div>
</div>
<p>To analyze error files and log files for possible error messages, use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR debug
</pre></div>
</div>
<p>By default, these commands will analyze the entire flow so the output on the terminal can be very verbose.
If you are interested in a particular task e.g. <code class="docutils literal"><span class="pre">w0/t1</span></code> use the syntax:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR/w0/t1 events
</pre></div>
</div>
<p>to select all the tasks in a work directory e.g. <code class="docutils literal"><span class="pre">w0</span></code> use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR/w0 events
</pre></div>
</div>
<p>to select an arbitrary subset of nodes of the flow use the syntax:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR events -nids=12,13,16
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">nids</span></code> is a list of AbiPy node identifiers.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last"><code class="docutils literal"><span class="pre">abirun.py</span> <span class="pre">events</span> <span class="pre">--help</span></code> is your best friend</p>
</div>
<div class="highlight-text"><div class="highlight"><pre><span></span>$ abirun.py events --help
usage: abirun.py [flowdir] events [-h] [-v] [--no-colors] [--no-logo]
                                  [--loglevel LOGLEVEL] [--remove-lock]
                                  [-n NIDS | -w WSLICE | -S TASK_STATUS]

optional arguments:
  -h, --help            show this help message and exit
  -v, --verbose         verbose, can be supplied multiple times to increase
                        verbosity.
  --no-colors           Disable ASCII colors.
  --no-logo             Disable AbiPy logo.
  --loglevel LOGLEVEL   set the loglevel. Possible values: CRITICAL, ERROR
                        (default), WARNING, INFO, DEBUG.
  --remove-lock         Remove the lock file of the pickle file storing the
                        flow.
  -n NIDS, --nids NIDS  Node identifier(s) used to select the task. Integer or
                        comma-separated list of integers. Use `status` command
                        to get the node ids. Examples: --nids=12
                        --nids=12,13,16 --nids=10:12 to select 10 and 11,
                        --nids=2:5:2 to select 2,4.
  -w WSLICE, --wslice WSLICE
                        Select the list of works to analyze (python syntax for
                        slices): Examples: --wslice=1 to select the second
                        workflow, --wslice=:3 for 0,1,2, --wslice=-1 for the
                        last workflow, --wslice::2 for even indices.
  -S TASK_STATUS, --task-status TASK_STATUS
                        Select only the tasks with the given status. Default:
                        None i.e. ignored. Possible values: [u&#39;Initialized&#39;,
                        u&#39;Locked&#39;, u&#39;Ready&#39;, u&#39;Submitted&#39;, u&#39;Running&#39;,
                        u&#39;Done&#39;, u&#39;AbiCritical&#39;, u&#39;QCritical&#39;, u&#39;Unconverged&#39;,
                        u&#39;Error&#39;, u&#39;Completed&#39;].
</pre></div>
</div>
<p>To get information on the Abinit executable used by the AbiPy, use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py abibuild
</pre></div>
</div>
<p>or the verbose version:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py abibuild --verbose
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">abirun.py</span></code> also provides tools to analyze the results of the flow at runtime.
The simplest command is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR tail
</pre></div>
</div>
<p>that is the analogous of Unix tail but a little bit more smarter in the
sense that <code class="docutils literal"><span class="pre">abirun.py</span></code> will only print to screen the final part of the output files
of the tasks that are <code class="docutils literal"><span class="pre">RUNNING</span></code>.</p>
<p>If you have <code class="docutils literal"><span class="pre">matplotlib</span></code> installed, you may want to use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py FLOWDIR inspect
</pre></div>
</div>
<p>Several AbiPy tasks, indeed, provide an <cite>inspect</cite> method that produces matplotlib figures
with data extracted from the output file.
For example, a <code class="docutils literal"><span class="pre">GsTask</span></code> prints the evolution of the ground-state SCF cycle.
The inspect command of <code class="docutils literal"><span class="pre">abirun.py</span></code> is just looping over the tasks of the flow and
call the <code class="docutils literal"><span class="pre">inspect</span></code> method.</p>
<p>The command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands notebook
</pre></div>
</div>
<p>generates a <code class="docutils literal"><span class="pre">jupyter</span></code> notebook with pre-defined python code that can be executed
to get a graphical representation of the status of our flow inside a web browser
(requires <code class="docutils literal"><span class="pre">jupyter</span></code>, <code class="docutils literal"><span class="pre">nbformat</span></code> and, obviously, a web browser).</p>
<p>Expert users may want to use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands ipython
</pre></div>
</div>
<p>to open the flow in the <code class="docutils literal"><span class="pre">ipython</span></code> shell to have direct access to the API provided by the flow.</p>
</div>
<div class="section" id="event-handlers">
<h2>Event handlers<a class="headerlink" href="#event-handlers" title="Permalink to this headline">¶</a></h2>
<p>An event handler is an action that will be executed in response of a particular event.
The AbiPy tasks have some built-in events handlers that will be executed to fix typical
runtime errors produced by Abinit.</p>
<p>To list the event handlers installed in a given flow use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$ abirun.py flow_si_ebands handlers
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">--verbose</span></code> option produces a more detailed description of the action performed
by the event handlers.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="gp">$</span> abirun.py flow_si_ebands handlers --verbose

<span class="go">List of event handlers installed:</span>
<span class="go">event name = !DilatmxError</span>
<span class="go">event documentation:</span>

<span class="go">    This Error occurs in variable cell calculations when the increase in the</span>
<span class="go">    unit cell volume is too large.</span>

<span class="go">handler documentation:</span>

<span class="go">    Handle DilatmxError. Abinit produces a netcdf file with the last structure before aborting</span>
<span class="go">    The handler changes the structure in the input with the last configuration and modify the value of dilatmx.</span>

<span class="go">event name = !TolSymError</span>
<span class="go">event documentation:</span>

<span class="go">    Class of errors raised by Abinit when it cannot detect the symmetries of the system.</span>
<span class="go">    The handler assumes the structure makes sense and the error is just due to numerical inaccuracies.</span>
<span class="go">    We increase the value of tolsym in the input file (default 1-8) so that Abinit can find the space group</span>
<span class="go">    and re-symmetrize the input structure.</span>

<span class="go">handler documentation:</span>

<span class="go">    Increase the value of tolsym in the input file.</span>

<span class="go">event name = !MemanaError</span>
<span class="go">event documentation:</span>

<span class="go">    Class of errors raised by the memory analyzer.</span>
<span class="go">    (the section that estimates the memory requirements from the input parameters).</span>

<span class="go">handler documentation:</span>

<span class="go">    Set mem_test to 0 to bypass the memory check.</span>

<span class="go">event name = !MemoryError</span>
<span class="go">event documentation:</span>

<span class="go">    This error occurs when a checked allocation fails in Abinit</span>
<span class="go">    The only way to go is to increase memory</span>

<span class="go">handler documentation:</span>

<span class="go">    Handle MemoryError. Increase the resources requirements</span>
</pre></div>
</div>
<p>At this point, you may wonder why we need to specify all these parameters in the configuration file.
The reason is that, before submitting a job to a resource manager, AbiPy will use the autoparal
feature of ABINIT to get all the possible parallel configurations with <cite>ncpus &lt;= max_cores</cite>.
On the basis of these results, <cite>AbiPy</cite> selects the &#8220;optimal&#8221; one, and changes the ABINIT input file
and the submission script accordingly .
(this is a very useful feature, especially for calculations done with <cite>paral_kgb=1</cite> that require
the specification of <code class="docutils literal"><span class="pre">npkpt</span></code>, <code class="docutils literal"><span class="pre">npfft</span></code>, <code class="docutils literal"><span class="pre">npband</span></code>, etc).
If more than one <cite>QueueAdapter</cite> is specified, AbiPy will first compute all the possible
configuration and then select the &#8220;optimal&#8221; <cite>QueueAdapter</cite> according to some kind of policy</p>
<p>In the previous sections, we have discussed how to define, build and run a <cite>Flow</cite>, but there is a very
important point that we haven&#8217;t discussed yet.
It should be stressed, indeed, that AbiPy is only driving and monitoring the <cite>Flow</cite> while the actual calculation
is delegated to ABINIT (a Fortran program that is usually executed in parallel on multiple CPUs that communicate
via the network by means of the MPI protocol).
Besides CPUs and memory must be reserved in advance by sending a request to the resource manager
installed on the clusters (SLURM, PBS, etc)</p>
</div>
<div class="section" id="taskpolicy">
<h2>TaskPolicy<a class="headerlink" href="#taskpolicy" title="Permalink to this headline">¶</a></h2>
<p>In some cases, you may want to enforce some constraint on the &#8220;optimal&#8221; configuration.
For example, you may want to select only those configurations whose parallel efficiency is greater than 0.7
and whose number of MPI nodes is divisible by 4.
One can easily enforce this constraint via the <cite>condition</cite> dictionary whose syntax is similar to the one used in <cite>mongodb</cite></p>
<div class="highlight-yaml"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">policy</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">autoparal</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="l l-Scalar l-Scalar-Plain">max_ncpus</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">10</span>
    <span class="l l-Scalar l-Scalar-Plain">condition</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="nv">$and</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span> <span class="p p-Indicator">{</span><span class="s">&quot;efficiency&quot;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="nv">$gt</span><span class="p p-Indicator">:</span> <span class="nv">0.7</span><span class="p p-Indicator">}},</span> <span class="p p-Indicator">{</span><span class="s">&quot;tot_ncpus&quot;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span><span class="nv">$divisible</span><span class="p p-Indicator">:</span> <span class="nv">4</span><span class="p p-Indicator">}}</span> <span class="p p-Indicator">]}</span>
</pre></div>
</div>
<p>The parallel efficiency is defined as $epsilon = dfrac{T_1}{T_N * N}$ where $N$ is the number
of MPI processes and $T_j$ is the wall time needed to complete the calculation with $j$ MPI processes.
For a perfect scaling implementation $epsilon$ is equal to one.
The parallel speedup with N processors is given by $S = T_N / T_1$.
Note that <code class="docutils literal"><span class="pre">autoparal</span> <span class="pre">=</span> <span class="pre">1</span></code> will automatically change your <code class="docutils literal"><span class="pre">job.sh</span></code> script as well as the input file
so that we can run the job in parallel with the optimal configuration required by the user.
For example, you can use <code class="docutils literal"><span class="pre">paral_kgb</span> <span class="pre">=</span> <span class="pre">1</span></code> in GS calculations and AbiPy will automatically set the values
of <code class="docutils literal"><span class="pre">npband</span></code>, <code class="docutils literal"><span class="pre">npfft</span></code>, <code class="docutils literal"><span class="pre">npkpt</span></code> ... for you!
Note that if no configuration fulfills the given condition, abipy will use the optimal configuration
that leads to the highest parallel speedup (not necessarily the most efficient one).</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="api/index.html" class="btn btn-neutral float-right" title="API documentation" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="examples/plot/plot_spectral_functions.html" class="btn btn-neutral" title="plot example code: plot_spectral_functions.py" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, The ABINIT group.
      Last updated on Mar 09, 2017.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>